{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Machine Learning Methods for Traffic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with supervised and unsupervised machine learning for classification purposes. \n",
    "\n",
    "The main tools used are python, pandas, scikit-learn and keras (with tensorflow as backend). \n",
    "\n",
    "Dealing with [NSL-KDD](https://www.unb.ca/cic/datasets/nsl.html) dataset as a concise representation of a real life network. With the aim of understanding, mining, processing and shaping the best model for the task of identifying incoming data packets, in real time, on a flow. \n",
    "\n",
    "After lots of attemps and approaches, I've included here some of the most relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Project Description](#1.-Project-Description)\n",
    "2. [Libraries and Versions](#2.-Libraries_and_Version)\n",
    "3. [Dataset Loading](#3.-Dataset_Loading)\n",
    "4. [Analytics and Visualization](#4.Analytics)\n",
    "5. [Pre-processing](#5.Pre-Processing)\n",
    "6. [Feature Selection and Dimensionality Reduction](#6.Feature_Selection_and_Dimensionality_Redcuction)\n",
    "7. [Classic Models](#7.Classic_Models)\n",
    "8. [Ensemble Approaches](#8.Ensemble_Approaches)\n",
    "9. [ANN Model](#9.ANN_Model)\n",
    "10. [Results and Conclusion](#10.Results_and_Conclusion) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internet is now something invisible and inherent to our civilization, like electricity. \n",
    "Almost every human being tends to spend part of his day on communications apps, devices or tools. \n",
    "We constantly applying our social skills thru the virtual realms.\n",
    "\n",
    "Computer networks are the backbone of all digital communications being made around the world today. Smartphones, notebooks, smart-homes and any connected device exchanges data thru one of these networks. Although they may use 'different' protocols, they are all subject of the same networks and routers. The internet traffic is huge. IPV6 is already working and supporting this massive information demand. There is something between 22 and 26 billion devices connected to the internet [1]. This numbers tend to grow massively as internet of things becomes the rule. More traffic means more treats, sophisticated viruses and malwares, zero-day attacks, and even, the now popular term, “cyber war”.\n",
    "\n",
    "It is not new to use machine learning techniques within offensive and defensive security areas. In this project, various classifiers are used to identify unknown and / or unwanted traffic. Their results are compared with each other and with the more common approaches, the signature-based systems. We choose a well-known dataset in the research of Intrusion Detection techniques, the NSL-KDD. \n",
    "\n",
    "An intrusion detection system is used to monitor the traffic in real time, classifying the treats per level. It can also become an autonomous system that drops these anomalous, inappropriate and / or unauthorized packets, an IPS (Intrusion Prevention System). An IPS can be seen as an extension of and IDS.\n",
    "\n",
    "Being a computer networks engineer and working on a company that develop devices for network defensive security based on dynamic automated over-the-air update signatures and rules, personal and professional motivations are just relevant and clear enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Libraries and Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versions for reproducibility\n",
    "------------------- -------\n",
    "- Package / Version\n",
    "<br>\n",
    "<br>Python 3.6.7 (needed for tensorflow)\n",
    "<br>\n",
    "<br>h5py                2.9.0\n",
    "<br>pandas              0.23.4\n",
    "<br>numpy               1.16.0\n",
    "<br>scipy               1.2.0\n",
    "<br>\n",
    "<br>ipykernel           5.1.0\n",
    "<br>ipython             7.2.0\n",
    "<br>\n",
    "<br>scikit-learn        0.20.2\n",
    "<br>\n",
    "<br>Keras               2.2.4\n",
    "<br>Keras-Applications  1.0.6\n",
    "<br>Keras-Preprocessing 1.0.5\n",
    "<br>\n",
    "<br>tensorboard         1.12.2\n",
    "<br>tensorflow-gpu      1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (Normalizer, \n",
    "                                   StandardScaler, \n",
    "                                   MinMaxScaler,\n",
    "                                   OneHotEncoder)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import (precision_score, \n",
    "                             recall_score,\n",
    "                             f1_score, \n",
    "                             accuracy_score,\n",
    "                             mean_squared_error,\n",
    "                             mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.utils import np_utils\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The [NSL-KDD](https://www.unb.ca/cic/datasets/nsl.html)\n",
    "is a prime tool for research and improvement of intrusion detection approaches. The good quality of this data presents an ideal scenario for offline works and benchmarks. The NSL-KDD dataset is an improved version of the KDD'99 dataset. \n",
    "    The inherent drawbacks of the KDD-cup'99 dataset has been, almost all, handled in the NSL-KDD. To address some of the corrections refinements we can quote:\n",
    "<br>\n",
    "- Redundant and / or duplicated records was removed from the train and the test set, which enable the classifiers with better detection rates to perform un-biased results on more frequent records. \n",
    "<br><br>\n",
    "- There are enough records in the train and test sets, which makes it affordable to run the experiments on the complete set without the need to randomly select a small portion or use some technique like random cross validation. This enables evaluation results of different works to be consistent and comparable.\n",
    "<br><br>\n",
    "- The number of selected records from each difficulty level group is inversely proportional to the percentage of records in the original KDD data set. As a result, the classification rates of distinct machine learning methods vary in a wider range, which makes it more efficient to have an accurate evaluation of different learning techniques.\n",
    "\n",
    "The NSL-KDD is not perfect. It still has some problems as it does not match exactly a real network. However, it is the maybe the most effective benchmark for the purpose of researching and comparing different methods and techniques for flow detection. \n",
    "-- --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set features and types\n",
    "\n",
    "On exploring the dataset, we can count forty-two attributes (plus the difficult level), that we will treat as features of the flow. Each one has an assigned label as a type of attack or normal traffic. We can access, besides what was considered above, detailed information of all 42 attributes, classes of the network connection (1 normal class and 4 attack class) and description of each attack class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125973 entries, 0 to 125972\n",
      "Data columns (total 43 columns):\n",
      "duration                       125973 non-null int64\n",
      "protocol_type                  125973 non-null object\n",
      "service                        125973 non-null object\n",
      "flag                           125973 non-null object\n",
      "src_bytes                      125973 non-null int64\n",
      "dst_bytes                      125973 non-null int64\n",
      "land                           125973 non-null int64\n",
      "wrong_fragment                 125973 non-null int64\n",
      "urgent                         125973 non-null int64\n",
      "hot                            125973 non-null int64\n",
      "num_failed_logins              125973 non-null int64\n",
      "logged_in                      125973 non-null int64\n",
      "num_compromised                125973 non-null int64\n",
      "root_shell                     125973 non-null int64\n",
      "su_attempted                   125973 non-null int64\n",
      "num_root                       125973 non-null int64\n",
      "num_file_creations             125973 non-null int64\n",
      "num_shells                     125973 non-null int64\n",
      "num_access_files               125973 non-null int64\n",
      "num_outbound_cmds              125973 non-null int64\n",
      "is_host_login                  125973 non-null int64\n",
      "is_guest_login                 125973 non-null int64\n",
      "count                          125973 non-null int64\n",
      "srv_count                      125973 non-null int64\n",
      "serror_rate                    125973 non-null float64\n",
      "srv_serror_rate                125973 non-null float64\n",
      "rerror_rate                    125973 non-null float64\n",
      "srv_rerror_rate                125973 non-null float64\n",
      "same_srv_rate                  125973 non-null float64\n",
      "diff_srv_rate                  125973 non-null float64\n",
      "srv_diff_host_rate             125973 non-null float64\n",
      "dst_host_count                 125973 non-null int64\n",
      "dst_host_srv_count             125973 non-null int64\n",
      "dst_host_same_srv_rate         125973 non-null float64\n",
      "dst_host_diff_srv_rate         125973 non-null float64\n",
      "dst_host_same_src_port_rate    125973 non-null float64\n",
      "dst_host_srv_diff_host_rate    125973 non-null float64\n",
      "dst_host_serror_rate           125973 non-null float64\n",
      "dst_host_srv_serror_rate       125973 non-null float64\n",
      "dst_host_rerror_rate           125973 non-null float64\n",
      "dst_host_srv_rerror_rate       125973 non-null float64\n",
      "class                          125973 non-null object\n",
      "difficult_level                125973 non-null int64\n",
      "dtypes: float64(15), int64(24), object(4)\n",
      "memory usage: 41.3+ MB\n"
     ]
    }
   ],
   "source": [
    "### NSL-KDD DataSet ###\n",
    "TRAIN = 'NSL-KDD/KDDTrain+.txt'\n",
    "TEST = 'NSL-KDD/KDDTest+.txt'\n",
    "\n",
    "# features / attributes / columns names\n",
    "feat_names = [\n",
    "     'duration',\n",
    "     'protocol_type',\n",
    "     'service',\n",
    "     'flag',\n",
    "     'src_bytes',\n",
    "     'dst_bytes',\n",
    "     'land',\n",
    "     'wrong_fragment',\n",
    "     'urgent',\n",
    "     'hot',\n",
    "     'num_failed_logins',\n",
    "     'logged_in',\n",
    "     'num_compromised',\n",
    "     'root_shell',\n",
    "     'su_attempted',\n",
    "     'num_root',\n",
    "     'num_file_creations',\n",
    "     'num_shells',\n",
    "     'num_access_files',\n",
    "     'num_outbound_cmds',\n",
    "     'is_host_login',\n",
    "     'is_guest_login', \n",
    "     'count',\n",
    "     'srv_count',    \n",
    "     'serror_rate',\n",
    "     'srv_serror_rate',\n",
    "     'rerror_rate',\n",
    "     'srv_rerror_rate',\n",
    "     'same_srv_rate',\n",
    "     'diff_srv_rate',\n",
    "     'srv_diff_host_rate',\n",
    "     'dst_host_count',\n",
    "     'dst_host_srv_count',\n",
    "     'dst_host_same_srv_rate',\n",
    "     'dst_host_diff_srv_rate',\n",
    "     'dst_host_same_src_port_rate',\n",
    "     'dst_host_srv_diff_host_rate',\n",
    "     'dst_host_serror_rate',\n",
    "     'dst_host_srv_serror_rate',\n",
    "     'dst_host_rerror_rate',\n",
    "     'dst_host_srv_rerror_rate',\n",
    "     'class',\n",
    "     'difficult_level'\n",
    "]\n",
    "\n",
    "# import dataset addind columns / features names\n",
    "df_train_raw = pd.read_csv(TRAIN, \n",
    "                       names=feat_names,\n",
    "                       index_col=False)\n",
    "\n",
    "df_test_raw = pd.read_csv(TEST, \n",
    "                      names=feat_names,\n",
    "                      index_col=False)\n",
    "df_train_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type of data (numeric, nominal, categorical, binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analize type of data and segment columns\n",
    "nominal = [1, 2, 3]\n",
    "binary = [6, 11, 13, 14, 20, 21]\n",
    "numeric = list(set(range(41)).difference(nominal).difference(binary))\n",
    "\n",
    "feat_names = np.array(feat_names)\n",
    "\n",
    "nominal_feats = feat_names[nominal].tolist()\n",
    "binary_feats = feat_names[binary].tolist()\n",
    "numeric_feats = feat_names[numeric].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic classification for train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "normal             67343\n",
       "neptune            41214\n",
       "satan               3633\n",
       "ipsweep             3599\n",
       "portsweep           2931\n",
       "smurf               2646\n",
       "nmap                1493\n",
       "back                 956\n",
       "teardrop             892\n",
       "warezclient          890\n",
       "pod                  201\n",
       "guess_passwd          53\n",
       "buffer_overflow       30\n",
       "warezmaster           20\n",
       "land                  18\n",
       "imap                  11\n",
       "rootkit               10\n",
       "loadmodule             9\n",
       "ftp_write              8\n",
       "multihop               7\n",
       "phf                    4\n",
       "perl                   3\n",
       "spy                    2\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Train Set')\n",
    "display(df_train_raw['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "normal             9711\n",
       "neptune            4657\n",
       "guess_passwd       1231\n",
       "mscan               996\n",
       "warezmaster         944\n",
       "apache2             737\n",
       "satan               735\n",
       "processtable        685\n",
       "smurf               665\n",
       "back                359\n",
       "snmpguess           331\n",
       "saint               319\n",
       "mailbomb            293\n",
       "snmpgetattack       178\n",
       "portsweep           157\n",
       "ipsweep             141\n",
       "httptunnel          133\n",
       "nmap                 73\n",
       "pod                  41\n",
       "buffer_overflow      20\n",
       "multihop             18\n",
       "named                17\n",
       "ps                   15\n",
       "sendmail             14\n",
       "xterm                13\n",
       "rootkit              13\n",
       "teardrop             12\n",
       "xlock                 9\n",
       "land                  7\n",
       "xsnoop                4\n",
       "ftp_write             3\n",
       "loadmodule            2\n",
       "perl                  2\n",
       "sqlattack             2\n",
       "udpstorm              2\n",
       "worm                  2\n",
       "phf                   2\n",
       "imap                  1\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test Set')\n",
    "display(df_test_raw['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maps traffic type into four attack classes / normal class. \n",
    "<br>Maps traffic for a binary classification (attack / normal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict that maps attacks to four groups for multiclass classification\n",
    "attack_types5 = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "# dict that maps attacks and normal traffic for binary classification\n",
    "attack_types2 = dict()\n",
    "for key, value in attack_types5.items():\n",
    "    attack_types2[key] = 'normal' if value == 'normal' else 'attack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps attack types (multiclass and binary) / train data\n",
    "df_train_01 = df_train_raw.copy()\n",
    "class_2 = df_train_01['class'].map(attack_types2).rename('class_2')\n",
    "class_5 = df_train_01['class'].map(attack_types5).rename('class_5')\n",
    "\n",
    "# drops features 'class' and 'difficult_level' as they will not be useful for this project\n",
    "df_train_01.drop(['class','difficult_level'], axis=1, inplace=True)\n",
    "df_train_01 = pd.concat([df_train_01, class_2, class_5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps attack types (multiclass and binary) / test data\n",
    "df_test_01 = df_test_raw.copy()\n",
    "class_2 = df_test_01['class'].map(attack_types2).rename('class_2')\n",
    "class_5 = df_test_01['class'].map(attack_types5).rename('class_5')\n",
    "\n",
    "# drops features 'class' and 'difficult_level' as they will not be useful for this project\n",
    "df_test_01.drop(['class','difficult_level'], axis=1, inplace=True)\n",
    "df_test_01 = pd.concat([df_test_01, class_2, class_5], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some numbers about the features / columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_id to groupby and counts\n",
    "count_id = range(df_train_01.shape[0])\n",
    "df_train_01.insert(43,'counts', count_id)\n",
    "\n",
    "count_id = range(df_test_01.shape[0])\n",
    "df_test_01.insert(43,'counts', count_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Attacks and Normal traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>class_2</th>\n",
       "      <th>attack</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>58630</td>\n",
       "      <td>67343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "class_2  attack  normal\n",
       "counts    58630   67343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attacks vs normal traffic (Train Data)\n",
    "display(df_train_01[['class_2', 'counts']].groupby(\n",
    "    'class_2').count().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>class_2</th>\n",
       "      <th>attack</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>12833</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "class_2  attack  normal\n",
       "counts    12833    9711"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attacks vs normal traffic (Test Data)\n",
    "display(df_test_01[['class_2', 'counts']].groupby(\n",
    "    'class_2').count().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>class_5</th>\n",
       "      <th>DoS</th>\n",
       "      <th>Probe</th>\n",
       "      <th>R2L</th>\n",
       "      <th>U2R</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>45927</td>\n",
       "      <td>11656</td>\n",
       "      <td>995</td>\n",
       "      <td>52</td>\n",
       "      <td>67343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "class_5    DoS  Probe  R2L  U2R  normal\n",
       "counts   45927  11656  995   52   67343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Types of attacks vs normal traffic (Train Data)\n",
    "display(df_train_01[['class_5', 'counts']].groupby(\n",
    "    'class_5').count().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>class_5</th>\n",
       "      <th>DoS</th>\n",
       "      <th>Probe</th>\n",
       "      <th>R2L</th>\n",
       "      <th>U2R</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>7458</td>\n",
       "      <td>2421</td>\n",
       "      <td>2754</td>\n",
       "      <td>200</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "class_5   DoS  Probe   R2L  U2R  normal\n",
       "counts   7458   2421  2754  200    9711"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Types of attacks vs normal traffic (Test Data)\n",
    "display(df_test_01[['class_5', 'counts']].groupby(\n",
    "    'class_5').count().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -  Nominal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>protocol_type</th>\n",
       "      <th>icmp</th>\n",
       "      <th>tcp</th>\n",
       "      <th>udp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>8291</td>\n",
       "      <td>102689</td>\n",
       "      <td>14993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "protocol_type  icmp     tcp    udp\n",
       "counts         8291  102689  14993"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nominal features ('protocol_type') /  3 unique items\n",
    "display(df_train_01[['protocol_type','counts']].groupby(\n",
    "    'protocol_type').count().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>service</th>\n",
       "      <th>IRC</th>\n",
       "      <th>X11</th>\n",
       "      <th>Z39_50</th>\n",
       "      <th>aol</th>\n",
       "      <th>auth</th>\n",
       "      <th>bgp</th>\n",
       "      <th>courier</th>\n",
       "      <th>csnet_ns</th>\n",
       "      <th>ctf</th>\n",
       "      <th>daytime</th>\n",
       "      <th>...</th>\n",
       "      <th>telnet</th>\n",
       "      <th>tftp_u</th>\n",
       "      <th>tim_i</th>\n",
       "      <th>time</th>\n",
       "      <th>urh_i</th>\n",
       "      <th>urp_i</th>\n",
       "      <th>uucp</th>\n",
       "      <th>uucp_path</th>\n",
       "      <th>vmnet</th>\n",
       "      <th>whois</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>187</td>\n",
       "      <td>73</td>\n",
       "      <td>862</td>\n",
       "      <td>2</td>\n",
       "      <td>955</td>\n",
       "      <td>710</td>\n",
       "      <td>734</td>\n",
       "      <td>545</td>\n",
       "      <td>563</td>\n",
       "      <td>521</td>\n",
       "      <td>...</td>\n",
       "      <td>2353</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>654</td>\n",
       "      <td>10</td>\n",
       "      <td>602</td>\n",
       "      <td>780</td>\n",
       "      <td>689</td>\n",
       "      <td>617</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "service  IRC  X11  Z39_50  aol  auth  bgp  courier  csnet_ns  ctf  daytime  \\\n",
       "counts   187   73     862    2   955  710      734       545  563      521   \n",
       "\n",
       "service  ...  telnet  tftp_u  tim_i  time  urh_i  urp_i  uucp  uucp_path  \\\n",
       "counts   ...    2353       3      8   654     10    602   780        689   \n",
       "\n",
       "service  vmnet  whois  \n",
       "counts     617    693  \n",
       "\n",
       "[1 rows x 70 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nominal features ('Services') / 70 unique items\n",
    "display(df_train_01[['service','counts']].groupby(\n",
    "    'service').count().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>flag</th>\n",
       "      <th>OTH</th>\n",
       "      <th>REJ</th>\n",
       "      <th>RSTO</th>\n",
       "      <th>RSTOS0</th>\n",
       "      <th>RSTR</th>\n",
       "      <th>S0</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>SF</th>\n",
       "      <th>SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>46</td>\n",
       "      <td>11233</td>\n",
       "      <td>1562</td>\n",
       "      <td>103</td>\n",
       "      <td>2421</td>\n",
       "      <td>34851</td>\n",
       "      <td>365</td>\n",
       "      <td>127</td>\n",
       "      <td>49</td>\n",
       "      <td>74945</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "flag    OTH    REJ  RSTO  RSTOS0  RSTR     S0   S1   S2  S3     SF   SH\n",
       "counts   46  11233  1562     103  2421  34851  365  127  49  74945  271"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nominal features ('flag') / 11 unique items\n",
    "display(df_train_01[['flag','counts']].groupby(\n",
    "    'flag').count().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -  Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.290800e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_bytes</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.379964e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_bytes</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.309937e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrong_fragment</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urgent</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hot</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.700000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_failed_logins</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_compromised</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.479000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_root</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.468000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_file_creations</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_shells</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_access_files</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_outbound_cmds</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.110000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srv_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.110000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serror_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rerror_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same_srv_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.550000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.550000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             min           max\n",
       "duration                     0.0  4.290800e+04\n",
       "src_bytes                    0.0  1.379964e+09\n",
       "dst_bytes                    0.0  1.309937e+09\n",
       "wrong_fragment               0.0  3.000000e+00\n",
       "urgent                       0.0  3.000000e+00\n",
       "hot                          0.0  7.700000e+01\n",
       "num_failed_logins            0.0  5.000000e+00\n",
       "num_compromised              0.0  7.479000e+03\n",
       "num_root                     0.0  7.468000e+03\n",
       "num_file_creations           0.0  4.300000e+01\n",
       "num_shells                   0.0  2.000000e+00\n",
       "num_access_files             0.0  9.000000e+00\n",
       "num_outbound_cmds            0.0  0.000000e+00\n",
       "count                        0.0  5.110000e+02\n",
       "srv_count                    0.0  5.110000e+02\n",
       "serror_rate                  0.0  1.000000e+00\n",
       "srv_serror_rate              0.0  1.000000e+00\n",
       "rerror_rate                  0.0  1.000000e+00\n",
       "srv_rerror_rate              0.0  1.000000e+00\n",
       "same_srv_rate                0.0  1.000000e+00\n",
       "diff_srv_rate                0.0  1.000000e+00\n",
       "srv_diff_host_rate           0.0  1.000000e+00\n",
       "dst_host_count               0.0  2.550000e+02\n",
       "dst_host_srv_count           0.0  2.550000e+02\n",
       "dst_host_same_srv_rate       0.0  1.000000e+00\n",
       "dst_host_diff_srv_rate       0.0  1.000000e+00\n",
       "dst_host_same_src_port_rate  0.0  1.000000e+00\n",
       "dst_host_srv_diff_host_rate  0.0  1.000000e+00\n",
       "dst_host_serror_rate         0.0  1.000000e+00\n",
       "dst_host_srv_serror_rate     0.0  1.000000e+00\n",
       "dst_host_rerror_rate         0.0  1.000000e+00\n",
       "dst_host_srv_rerror_rate     0.0  1.000000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train_01[numeric_feats].describe().transpose()[['min','max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature 'num_outbound_cmds' has only 0.0 values, \n",
    "<br>so it is ok to drop it on both train and test sets: num_outbound_cmds\tMIN: 0.0\tMAX: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop from train and test sets\n",
    "df_train_01.drop(['num_outbound_cmds'], axis=1, inplace=True)\n",
    "df_test_01.drop(['num_outbound_cmds'], axis=1, inplace=True)\n",
    "\n",
    "# remove from numeric features list\n",
    "numeric_feats.remove('num_outbound_cmds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -  Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>land</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logged_in</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>root_shell</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>su_attempted</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_host_login</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_guest_login</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                min  max\n",
       "land            0.0  1.0\n",
       "logged_in       0.0  1.0\n",
       "root_shell      0.0  1.0\n",
       "su_attempted    0.0  2.0\n",
       "is_host_login   0.0  1.0\n",
       "is_guest_login  0.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train_01[binary_feats].describe().transpose()[['min','max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature 'su_attempted' has 3 values, but it should be binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train_01['su_attempted'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# replace the value 2.0 for 0.0 on the feature 'su_attempted' on both sets\n",
    "df_train_01['su_attempted'] = df_train_01['su_attempted'].replace(2, 0)\n",
    "df_test_01['su_attempted'] = df_test_01['su_attempted'].replace(2, 0)\n",
    "\n",
    "display(df_train_01['su_attempted'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get all categorical features and one hot encode them. \n",
    "<br>Can be done with sklearn OneHotEncoder or any other OHE function. \n",
    "<br>Here, pandas get_dummies on a custom function gets the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for categorical features\n",
    "def ohe(df, cols_and_prefs):\n",
    "    return [pd.get_dummies(df[col], prefix=prefix) for col,prefix in cols_and_prefs]\n",
    "\n",
    "# Categorical features and prefixes for newly one hot encoded ones\n",
    "cols_and_prefs = [('protocol_type','proto'), ('service','serv'), ('flag','flag')]\n",
    "df_train_ohe = ohe(df_train_01[nominal_feats], cols_and_prefs)\n",
    "df_test_ohe = ohe(df_test_01[nominal_feats], cols_and_prefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop 'counts' column from both datasets. (only used to do anaytics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df_train_01.drop(['counts'], axis=1, inplace=True)\n",
    "\n",
    "# Test data\n",
    "df_test_01.drop(['counts'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function to remove categorical columns and add encoded ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove old columns and add ohe / encoded ones    \n",
    "def del_add_cols(df, cols_and_prefs, cols_list):\n",
    "    temp_df = df.copy()\n",
    "    del_list = [col[0] for col in cols_and_prefs]\n",
    "    for col in del_list:\n",
    "        temp_df.drop([col], axis=1, inplace=True)\n",
    "    for new_col in cols_list:\n",
    "        temp_df = pd.concat([temp_df, new_col], axis=1)\n",
    "    return temp_df\n",
    "\n",
    "# Train data\n",
    "df_train_02 = del_add_cols(df_train_01, cols_and_prefs, df_train_ohe)\n",
    "    \n",
    "# Test data\n",
    "df_test_02 = del_add_cols(df_test_01, cols_and_prefs, df_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one hot encoding, there is six features found only on the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['serv_aol',\n",
       " 'serv_harvest',\n",
       " 'serv_http_2784',\n",
       " 'serv_http_8001',\n",
       " 'serv_red_i',\n",
       " 'serv_urh_i']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_train_02.columns.difference(df_test_02.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding empty columns / zero value rows (for missing data), to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_cols = list(df_train_02.columns.difference(df_test_02.columns))\n",
    "for col in diff_cols:\n",
    "    df_test_02[col] = 0\n",
    "    \n",
    "df_train_02.columns.difference(df_test_02.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenting datasets in five parts to each class of traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data segmented per attack class / normal traffic\n",
    "df_train_probe = df_train_02[df_train_02['class_5'].isin(['normal', 'Probe'])].copy()\n",
    "df_train_dos = df_train_02[df_train_02['class_5'].isin(['normal', 'DoS'])].copy()\n",
    "df_train_u2r = df_train_02[df_train_02['class_5'].isin(['normal', 'U2R'])].copy()\n",
    "df_train_r2l = df_train_02[df_train_02['class_5'].isin(['normal', 'R2L'])].copy()\n",
    "\n",
    "# test data segmented per attack class / normal traffic\n",
    "df_test_probe = df_test_02[df_test_02['class_5'].isin(['normal', 'Probe'])].copy()\n",
    "df_test_dos = df_test_02[df_test_02['class_5'].isin(['normal', 'DoS'])].copy()\n",
    "df_test_u2r = df_test_02[df_test_02['class_5'].isin(['normal', 'U2R'])].copy()\n",
    "df_test_r2l = df_test_02[df_test_02['class_5'].isin(['normal', 'R2L'])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "targets for train and test data, for each class of traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass dict\n",
    "attack_types_num = {'normal': 0, 'Probe': 1, 'DoS': 2, 'U2R': 3, 'R2L': 4}\n",
    "\n",
    "# all data\n",
    "df_train = df_train_02.copy()\n",
    "target_train2 = df_train['class_2'].map({'normal': 0, 'attack': 1}).rename('label')\n",
    "target_train5 = df_train['class_5'].map(attack_types_num).rename('label')\n",
    "df_train.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "\n",
    "df_test = df_test_02.copy()\n",
    "target_test2 = df_test['class_2'].map({'normal': 0, 'attack': 1}).rename('label')\n",
    "target_test5 = df_test['class_5'].map(attack_types_num).rename('label')\n",
    "df_test.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "\n",
    "# segmented train data\n",
    "target_train_probe = df_train_probe['class_5'].map({'normal':0, 'Probe': 1}).rename('probe')\n",
    "target_train_dos = df_train_dos['class_5'].map({'normal':0, 'DoS': 1}).rename('probe')\n",
    "target_train_u2r = df_train_u2r['class_5'].map({'normal':0, 'U2R': 1}).rename('probe')\n",
    "target_train_r2l = df_train_r2l['class_5'].map({'normal':0, 'R2L': 1}).rename('probe')\n",
    "\n",
    "df_train_probe.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "df_train_dos.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "df_train_u2r.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "df_train_r2l.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "\n",
    "# segmented target data\n",
    "target_test_probe = df_test_probe['class_5'].map({'normal':0, 'Probe': 1}).rename('probe')\n",
    "target_test_dos = df_test_dos['class_5'].map({'normal':0, 'DoS': 1}).rename('probe')\n",
    "target_test_u2r = df_test_u2r['class_5'].map({'normal':0, 'U2R': 1}).rename('probe')\n",
    "target_test_r2l = df_test_r2l['class_5'].map({'normal':0, 'R2L': 1}).rename('probe')\n",
    "\n",
    "df_test_probe.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "df_test_dos.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "df_test_u2r.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "df_test_r2l.drop(['class_2', 'class_5'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data\n",
    "X = df_train\n",
    "y2 = target_train2\n",
    "y5 = target_train5\n",
    "X_train = np.array(Normalizer().fit_transform(X))\n",
    "y_train2 = np.array(y2)\n",
    "y_train5 = np.array(y5)\n",
    "\n",
    "X = df_test\n",
    "y2 = target_test2\n",
    "y5 = target_test5\n",
    "X_test = np.array(Normalizer().fit_transform(X))\n",
    "y_test2 = np.array(y2)\n",
    "y_test5 = np.array(y5)\n",
    "\n",
    "\n",
    "# Probe traffic\n",
    "X = df_train_probe\n",
    "y = target_train_probe\n",
    "X_train_probe = np.array(Normalizer().fit_transform(X))\n",
    "y_train_probe = np.array(y)\n",
    "\n",
    "X = df_test_probe\n",
    "y = target_test_probe\n",
    "X_test_probe = np.array(Normalizer().fit_transform(X))\n",
    "y_test_probe = np.array(y)\n",
    "\n",
    "\n",
    "# DoS traffic\n",
    "X = df_train_dos\n",
    "y = target_train_dos\n",
    "X_train_dos = np.array(Normalizer().fit_transform(X))\n",
    "y_train_dos = np.array(y)\n",
    "\n",
    "X = df_test_dos\n",
    "y = target_test_dos\n",
    "X_test_dos = np.array(Normalizer().fit_transform(X))\n",
    "y_test_dos = np.array(y)\n",
    "\n",
    "\n",
    "# U2R traffic\n",
    "X = df_train_u2r\n",
    "y = target_train_u2r\n",
    "X_train_u2r = np.array(Normalizer().fit_transform(X))\n",
    "y_train_u2r = np.array(y)\n",
    "\n",
    "X = df_test_u2r\n",
    "y = target_test_u2r\n",
    "X_test_u2r = np.array(Normalizer().fit_transform(X))\n",
    "y_test_u2r = np.array(y)\n",
    "\n",
    "\n",
    "# R2L traffic\n",
    "X = df_train_r2l\n",
    "y = target_train_r2l\n",
    "X_train_r2l = np.array(Normalizer().fit_transform(X))\n",
    "y_train_r2l = np.array(y)\n",
    "\n",
    "X = df_test_r2l\n",
    "y = target_test_r2l\n",
    "X_test_r2l = np.array(Normalizer().fit_transform(X))\n",
    "y_test_r2l = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 features / columns are selected for dimensionality reduction purposes.\n",
    "<br>It means around 16% of total features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_probe = SelectKBest(chi2, k=20)\n",
    "kb_probe.fit_transform(X_train_probe, target_train_probe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'rerror_rate',\n",
       " 'srv_rerror_rate',\n",
       " 'same_srv_rate',\n",
       " 'srv_diff_host_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_srv_rate',\n",
       " 'dst_host_diff_srv_rate',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'dst_host_srv_diff_host_rate',\n",
       " 'dst_host_srv_rerror_rate',\n",
       " 'proto_icmp',\n",
       " 'serv_eco_i',\n",
       " 'serv_private',\n",
       " 'flag_SF']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true = kb_probe.get_support()\n",
    "ix_probe = [i for i,x in enumerate(true) if x]\n",
    "\n",
    "X_train_probe_kb = df_train_probe.iloc[:, ix_probe]\n",
    "X_test_probe_kb = df_test_probe.iloc[:, ix_probe]\n",
    "display(list(X_train_probe_kb.columns))\n",
    "\n",
    "# normalize selected features\n",
    "X_train_probe_kb = np.array(Normalizer().fit_transform(X_train_probe_kb))\n",
    "X_test_probe_kb = np.array(Normalizer().fit_transform(X_test_probe_kb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DoS selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_dos = SelectKBest(chi2, k=20)\n",
    "kb_dos.fit_transform(X_train_dos, target_train_dos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'logged_in',\n",
       " 'count',\n",
       " 'serror_rate',\n",
       " 'srv_serror_rate',\n",
       " 'same_srv_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_srv_rate',\n",
       " 'dst_host_serror_rate',\n",
       " 'dst_host_srv_serror_rate',\n",
       " 'proto_tcp',\n",
       " 'proto_udp',\n",
       " 'serv_domain_u',\n",
       " 'serv_http',\n",
       " 'serv_private',\n",
       " 'flag_S0',\n",
       " 'flag_SF']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true = kb_dos.get_support()\n",
    "ix_dos = [i for i,x in enumerate(true) if x]\n",
    "\n",
    "X_train_dos_kb = df_train_dos.iloc[:, ix_dos]\n",
    "X_test_dos_kb = df_test_dos.iloc[:, ix_dos]\n",
    "display(list(X_train_dos_kb.columns))\n",
    "\n",
    "# normalize selected features\n",
    "X_train_dos_kb = np.array(Normalizer().fit_transform(X_train_dos_kb))\n",
    "X_test_dos_kb = np.array(Normalizer().fit_transform(X_test_dos_kb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U2R selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_u2r = SelectKBest(chi2, k=20)\n",
    "kb_u2r.fit_transform(X_train_u2r, target_train_u2r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['src_bytes',\n",
       " 'dst_bytes',\n",
       " 'hot',\n",
       " 'num_failed_logins',\n",
       " 'num_compromised',\n",
       " 'root_shell',\n",
       " 'num_file_creations',\n",
       " 'num_shells',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'same_srv_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_srv_rate',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'proto_udp',\n",
       " 'serv_http',\n",
       " 'serv_other',\n",
       " 'serv_telnet',\n",
       " 'flag_SF']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true = kb_u2r.get_support()\n",
    "ix_u2r = [i for i,x in enumerate(true) if x]\n",
    "\n",
    "X_train_u2r_kb = df_train_u2r.iloc[:, ix_u2r]\n",
    "X_test_u2r_kb = df_test_u2r.iloc[:, ix_u2r]\n",
    "display(list(X_train_u2r_kb.columns))\n",
    "\n",
    "# normalize selected features\n",
    "X_train_u2r_kb = np.array(Normalizer().fit_transform(X_train_u2r_kb))\n",
    "X_test_u2r_kb = np.array(Normalizer().fit_transform(X_test_u2r_kb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2L selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_r2l = SelectKBest(chi2, k=20)\n",
    "kb_r2l.fit_transform(X_train_r2l, target_train_r2l);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'hot',\n",
       " 'num_failed_logins',\n",
       " 'logged_in',\n",
       " 'is_guest_login',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'proto_tcp',\n",
       " 'proto_udp',\n",
       " 'serv_ftp',\n",
       " 'serv_ftp_data',\n",
       " 'serv_http',\n",
       " 'serv_imap4',\n",
       " 'serv_telnet',\n",
       " 'flag_RSTO']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true = kb_r2l.get_support()\n",
    "ix_r2l = [i for i,x in enumerate(true) if x]\n",
    "\n",
    "X_train_r2l_kb = df_train_r2l.iloc[:, ix_r2l]\n",
    "X_test_r2l_kb = df_test_r2l.iloc[:, ix_r2l]\n",
    "display(list(X_train_r2l_kb.columns))\n",
    "\n",
    "# normalize selected features\n",
    "X_train_r2l_kb = np.array(Normalizer().fit_transform(X_train_r2l_kb))\n",
    "X_test_r2l_kb = np.array(Normalizer().fit_transform(X_test_r2l_kb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selected features considering multi and binary classifications and for all kinds of traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'wrong_fragment',\n",
       " 'logged_in',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'serror_rate',\n",
       " 'srv_serror_rate',\n",
       " 'rerror_rate',\n",
       " 'srv_rerror_rate',\n",
       " 'diff_srv_rate',\n",
       " 'srv_diff_host_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_diff_srv_rate',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'dst_host_srv_diff_host_rate',\n",
       " 'dst_host_serror_rate',\n",
       " 'dst_host_srv_serror_rate',\n",
       " 'dst_host_rerror_rate',\n",
       " 'dst_host_srv_rerror_rate',\n",
       " 'proto_icmp',\n",
       " 'proto_tcp',\n",
       " 'proto_udp',\n",
       " 'serv_domain_u',\n",
       " 'serv_eco_i',\n",
       " 'serv_http',\n",
       " 'serv_private',\n",
       " 'flag_S0']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for binary classification\n",
    "kb_train2 = SelectKBest(chi2, k=30)\n",
    "kb_train2.fit_transform(X_train, target_train2);\n",
    "\n",
    "true = kb_train2.get_support()\n",
    "ix_probe = [i for i,x in enumerate(true) if x]\n",
    "\n",
    "X_train_kb2 = df_train.iloc[:, ix_probe]\n",
    "X_test_kb2 = df_test.iloc[:, ix_probe]\n",
    "display(list(X_train_kb2.columns))\n",
    "\n",
    "# normalize selected features\n",
    "X_train_kb2 = np.array(Normalizer().fit_transform(X_train_kb2))\n",
    "X_test_kb2 = np.array(Normalizer().fit_transform(X_test_kb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'hot',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'serror_rate',\n",
       " 'srv_serror_rate',\n",
       " 'rerror_rate',\n",
       " 'srv_rerror_rate',\n",
       " 'same_srv_rate',\n",
       " 'srv_diff_host_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_srv_rate',\n",
       " 'dst_host_diff_srv_rate',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'dst_host_srv_diff_host_rate',\n",
       " 'dst_host_serror_rate',\n",
       " 'dst_host_srv_serror_rate',\n",
       " 'dst_host_rerror_rate',\n",
       " 'dst_host_srv_rerror_rate',\n",
       " 'proto_icmp',\n",
       " 'proto_tcp',\n",
       " 'serv_eco_i',\n",
       " 'serv_private',\n",
       " 'flag_REJ',\n",
       " 'flag_RSTR',\n",
       " 'flag_S0',\n",
       " 'flag_SF']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for multi classification\n",
    "kb_train5 = SelectKBest(chi2, k=30)\n",
    "kb_train5.fit_transform(X_train, target_train5);\n",
    "\n",
    "true = kb_train5.get_support()\n",
    "ix_probe = [i for i,x in enumerate(true) if x]\n",
    "\n",
    "X_train_kb5 = df_train.iloc[:, ix_probe]\n",
    "X_test_kb5 = df_test.iloc[:, ix_probe]\n",
    "display(list(X_train_kb5.columns))\n",
    "\n",
    "# normalize selected features\n",
    "X_train_kb5 = np.array(Normalizer().fit_transform(X_train_kb5))\n",
    "X_test_kb5 = np.array(Normalizer().fit_transform(X_test_kb5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic classifiers used for binary classification\n",
    "<br>between 'normal' and 'attack' for each group of traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_clfs = [('Naive Bayes', MultinomialNB()),\n",
    "                ('Decision Trees', DecisionTreeClassifier(max_depth=18, \n",
    "                                                          min_samples_leaf=2, \n",
    "                                                          min_samples_split=10)),\n",
    "                ('K-Nearest Neighbor', KNeighborsClassifier(n_neighbors=2,\n",
    "                                                            weights='distance')),\n",
    "                ('SVM', SVC(C=1000, kernel='linear'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters used above was obteined thru greedsearchCV.\n",
    "<br>It can be accecced on helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = [('accuracy', accuracy_score), ('precision', precision_score)]\n",
    "x_val_scorers = ['accuracy', 'precision']\n",
    "\n",
    "\n",
    "# function to apply different classifiers on segmented data \n",
    "def classifiers(X_train, y_train, X_test, y_test, clfs, scorers=scorers, x_val_scorers=x_val_scorers):\n",
    "    results = dict()\n",
    "    for name,clf in clfs:\n",
    "        clf.fit(X_train, y_train)\n",
    "        predict = clf.predict(X_test)\n",
    "        results[name] = dict()\n",
    "        for s_name,score in scorers:\n",
    "            results[name][s_name] = score(y_test, predict) \n",
    "        for x_score in x_val_scorers:\n",
    "            results[name]['cross validation '+x_score] = cross_val_score(clf, X_test, y_test, \n",
    "                                                                         cv=10, scoring=x_score).mean()\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results_dict):\n",
    "    bold = \"\\033[1m\"\n",
    "    reset = \"\\033[0;0m\"\n",
    "    for key,value in results_dict.items():\n",
    "        print(bold,f'\\n{key}:\\n',reset)\n",
    "        for k,v in value.items():\n",
    "            print(f'{k}: {v:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_results = classifiers(X_train_probe, y_train_probe,\n",
    "                            X_test_probe, y_test_probe,\n",
    "                            classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 85.24%\n",
      "precision: 96.74%\n",
      "cross validation accuracy: 87.26%\n",
      "cross validation precision: 90.21%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 88.26%\n",
      "precision: 68.23%\n",
      "cross validation accuracy: 99.37%\n",
      "cross validation precision: 98.72%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 91.45%\n",
      "precision: 87.57%\n",
      "cross validation accuracy: 98.90%\n",
      "cross validation precision: 96.91%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 86.88%\n",
      "precision: 69.62%\n",
      "cross validation accuracy: 97.77%\n",
      "cross validation precision: 91.87%\n"
     ]
    }
   ],
   "source": [
    "print_results(probe_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_results_kb = classifiers(X_train_probe_kb, y_train_probe,\n",
    "                               X_test_probe_kb, y_test_probe,\n",
    "                               classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 85.14%\n",
      "precision: 96.82%\n",
      "cross validation accuracy: 87.00%\n",
      "cross validation precision: 89.62%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 88.17%\n",
      "precision: 89.00%\n",
      "cross validation accuracy: 99.07%\n",
      "cross validation precision: 98.01%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 91.15%\n",
      "precision: 87.31%\n",
      "cross validation accuracy: 98.54%\n",
      "cross validation precision: 96.20%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 86.42%\n",
      "precision: 70.97%\n",
      "cross validation accuracy: 97.32%\n",
      "cross validation precision: 91.15%\n"
     ]
    }
   ],
   "source": [
    "print_results(probe_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOS data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_results = classifiers(X_train_dos, y_train_dos,\n",
    "                          X_test_dos, y_test_dos,\n",
    "                          classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 80.19%\n",
      "precision: 86.22%\n",
      "cross validation accuracy: 84.75%\n",
      "cross validation precision: 85.71%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 84.87%\n",
      "precision: 98.99%\n",
      "cross validation accuracy: 99.62%\n",
      "cross validation precision: 99.66%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 87.80%\n",
      "precision: 96.29%\n",
      "cross validation accuracy: 99.11%\n",
      "cross validation precision: 98.68%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 87.44%\n",
      "precision: 98.79%\n",
      "cross validation accuracy: 96.83%\n",
      "cross validation precision: 93.42%\n"
     ]
    }
   ],
   "source": [
    "print_results(dos_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_results_kb = classifiers(X_train_dos_kb, y_train_dos,\n",
    "                             X_test_dos_kb, y_test_dos,\n",
    "                             classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 80.14%\n",
      "precision: 86.27%\n",
      "cross validation accuracy: 84.57%\n",
      "cross validation precision: 85.30%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 90.82%\n",
      "precision: 98.20%\n",
      "cross validation accuracy: 99.60%\n",
      "cross validation precision: 99.57%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 87.90%\n",
      "precision: 96.78%\n",
      "cross validation accuracy: 99.02%\n",
      "cross validation precision: 98.68%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 80.60%\n",
      "precision: 83.98%\n",
      "cross validation accuracy: 96.21%\n",
      "cross validation precision: 92.34%\n"
     ]
    }
   ],
   "source": [
    "print_results(dos_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U2R data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "u2r_results = classifiers(X_train_u2r, y_train_u2r,\n",
    "                          X_test_u2r, y_test_u2r,\n",
    "                          classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 97.98%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 97.98%\n",
      "cross validation precision: 0.00%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 98.07%\n",
      "precision: 80.00%\n",
      "cross validation accuracy: 99.65%\n",
      "cross validation precision: 96.43%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 98.06%\n",
      "precision: 78.57%\n",
      "cross validation accuracy: 99.11%\n",
      "cross validation precision: 79.43%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 97.98%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 99.07%\n",
      "cross validation precision: 99.09%\n"
     ]
    }
   ],
   "source": [
    "print_results(u2r_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "u2r_results_kb = classifiers(X_train_u2r_kb, y_train_u2r,\n",
    "                             X_test_u2r_kb, y_test_u2r,\n",
    "                             classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 97.98%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 97.98%\n",
      "cross validation precision: 0.00%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 98.08%\n",
      "precision: 81.25%\n",
      "cross validation accuracy: 99.03%\n",
      "cross validation precision: 77.36%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 98.02%\n",
      "precision: 83.33%\n",
      "cross validation accuracy: 98.63%\n",
      "cross validation precision: 67.07%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 97.98%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 97.98%\n",
      "cross validation precision: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print_results(u2r_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2L data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "r2l_results = classifiers(X_train_r2l, y_train_r2l,\n",
    "                          X_test_r2l, y_test_r2l,\n",
    "                          classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 77.91%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 77.75%\n",
      "cross validation precision: 28.22%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 78.48%\n",
      "precision: 95.00%\n",
      "cross validation accuracy: 97.49%\n",
      "cross validation precision: 95.20%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 81.65%\n",
      "precision: 98.95%\n",
      "cross validation accuracy: 97.27%\n",
      "cross validation precision: 93.91%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 78.50%\n",
      "precision: 94.05%\n",
      "cross validation accuracy: 95.23%\n",
      "cross validation precision: 89.73%\n"
     ]
    }
   ],
   "source": [
    "print_results(r2l_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "r2l_results_kb = classifiers(X_train_r2l_kb, y_train_r2l,\n",
    "                             X_test_r2l_kb, y_test_r2l,\n",
    "                             classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 77.91%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 77.73%\n",
      "cross validation precision: 25.56%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 78.59%\n",
      "precision: 96.70%\n",
      "cross validation accuracy: 97.48%\n",
      "cross validation precision: 94.72%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 81.57%\n",
      "precision: 98.93%\n",
      "cross validation accuracy: 97.15%\n",
      "cross validation precision: 93.62%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 77.94%\n",
      "precision: 83.33%\n",
      "cross validation accuracy: 92.60%\n",
      "cross validation precision: 84.36%\n"
     ]
    }
   ],
   "source": [
    "print_results(r2l_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All data binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_bin_results = classifiers(X_train, y_train2,\n",
    "                                    X_test, y_test2,\n",
    "                                    classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 70.88%\n",
      "precision: 86.66%\n",
      "cross validation accuracy: 82.41%\n",
      "cross validation precision: 82.82%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 60.48%\n",
      "precision: 84.09%\n",
      "cross validation accuracy: 98.01%\n",
      "cross validation precision: 98.24%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 77.86%\n",
      "precision: 96.83%\n",
      "cross validation accuracy: 97.24%\n",
      "cross validation precision: 97.42%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 70.10%\n",
      "precision: 90.06%\n",
      "cross validation accuracy: 92.81%\n",
      "cross validation precision: 90.53%\n"
     ]
    }
   ],
   "source": [
    "print_results(all_class_bin_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_bin_results_kb = classifiers(X_train_kb2, y_train2,\n",
    "                                       X_test_kb2, y_test2,\n",
    "                                       classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 70.83%\n",
      "precision: 86.64%\n",
      "cross validation accuracy: 82.34%\n",
      "cross validation precision: 82.79%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 75.02%\n",
      "precision: 96.66%\n",
      "cross validation accuracy: 97.91%\n",
      "cross validation precision: 98.33%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 77.62%\n",
      "precision: 96.76%\n",
      "cross validation accuracy: 97.00%\n",
      "cross validation precision: 97.23%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 67.83%\n",
      "precision: 86.97%\n",
      "cross validation accuracy: 91.63%\n",
      "cross validation precision: 90.60%\n"
     ]
    }
   ],
   "source": [
    "print_results(all_class_bin_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All data multi classification\n",
    "<br> change average to 'macro' due to multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifiers_macro(X_train, y_train, X_test, y_test, clfs, scorers=scorers, x_val_scorers=x_val_scorers):\n",
    "    results = dict()\n",
    "    for name,clf in clfs:\n",
    "        clf.fit(X_train, y_train)\n",
    "        predict = clf.predict(X_test)\n",
    "        results[name] = dict()\n",
    "        for s_name,score in scorers:\n",
    "            if s_name == 'precision':\n",
    "                results[name][s_name] = score(y_test, predict, average='macro') \n",
    "            else:\n",
    "                results[name][s_name] = score(y_test, predict) \n",
    "            results[name]['cross validation accuracy'] = cross_val_score(clf, X_test, y_test, \n",
    "                                                                         cv=10, scoring='accuracy').mean()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_results = classifiers_macro(X_train, y_train5,\n",
    "                                        X_test, y_test5,\n",
    "                                        classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 61.08%\n",
      "cross validation accuracy: 64.56%\n",
      "precision: 33.88%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 71.89%\n",
      "cross validation accuracy: 97.82%\n",
      "precision: 77.24%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 75.18%\n",
      "cross validation accuracy: 96.73%\n",
      "precision: 77.58%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 71.88%\n",
      "cross validation accuracy: 93.46%\n",
      "precision: 49.77%\n"
     ]
    }
   ],
   "source": [
    "print_results(multi_class_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "multi_class_results_kb = classifiers_macro(X_train_kb5, y_train5,\n",
    "                                           X_test_kb5, y_test5,\n",
    "                                           classic_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Naive Bayes:\n",
      " \u001b[0;0m\n",
      "accuracy: 61.05%\n",
      "cross validation accuracy: 64.50%\n",
      "precision: 32.48%\n",
      "\u001b[1m \n",
      "Decision Trees:\n",
      " \u001b[0;0m\n",
      "accuracy: 71.38%\n",
      "cross validation accuracy: 97.20%\n",
      "precision: 71.19%\n",
      "\u001b[1m \n",
      "K-Nearest Neighbor:\n",
      " \u001b[0;0m\n",
      "accuracy: 74.88%\n",
      "cross validation accuracy: 96.32%\n",
      "precision: 78.17%\n",
      "\u001b[1m \n",
      "SVM:\n",
      " \u001b[0;0m\n",
      "accuracy: 69.11%\n",
      "cross validation accuracy: 91.40%\n",
      "precision: 65.91%\n"
     ]
    }
   ],
   "source": [
    "print_results(multi_class_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble classifiers used for binary classification\n",
    "between 'normal' and 'attack' for each group of traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_clfs = [('Adaboost', \n",
    "                 AdaBoostClassifier(base_estimator=DecisionTreeClassifier( \n",
    "                                    max_depth=18, \n",
    "                                    min_samples_leaf=2, \n",
    "                                    min_samples_split=10),\n",
    "                                    n_estimators=100)),\n",
    "                ('Random Forest', \n",
    "                 RandomForestClassifier(n_estimators=100, \n",
    "                                        max_depth=18, \n",
    "                                        min_samples_leaf=2, \n",
    "                                        min_samples_split=10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_results = classifiers(X_train_probe, y_train_probe,\n",
    "                            X_test_probe, y_test_probe,\n",
    "                            ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 89.06%\n",
      "precision: 85.75%\n",
      "cross validation accuracy: 99.69%\n",
      "cross validation precision: 99.42%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 90.22%\n",
      "precision: 87.67%\n",
      "cross validation accuracy: 99.53%\n",
      "cross validation precision: 98.85%\n"
     ]
    }
   ],
   "source": [
    "print_results(probe_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_results_kb = classifiers(X_train_probe_kb, y_train_probe,\n",
    "                               X_test_probe_kb, y_test_probe,\n",
    "                               ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 88.78%\n",
      "precision: 88.02%\n",
      "cross validation accuracy: 99.36%\n",
      "cross validation precision: 98.59%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 88.25%\n",
      "precision: 85.93%\n",
      "cross validation accuracy: 99.23%\n",
      "cross validation precision: 97.91%\n"
     ]
    }
   ],
   "source": [
    "print_results(probe_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOS data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_results = classifiers(X_train_dos, y_train_dos,\n",
    "                          X_test_dos, y_test_dos,\n",
    "                          ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 87.77%\n",
      "precision: 97.22%\n",
      "cross validation accuracy: 99.84%\n",
      "cross validation precision: 99.87%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 85.72%\n",
      "precision: 99.39%\n",
      "cross validation accuracy: 99.81%\n",
      "cross validation precision: 99.89%\n"
     ]
    }
   ],
   "source": [
    "print_results(dos_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_results_kb = classifiers(X_train_dos_kb, y_train_dos,\n",
    "                             X_test_dos_kb, y_test_dos,\n",
    "                             ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 77.57%\n",
      "precision: 99.34%\n",
      "cross validation accuracy: 99.79%\n",
      "cross validation precision: 99.80%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 88.21%\n",
      "precision: 97.55%\n",
      "cross validation accuracy: 99.79%\n",
      "cross validation precision: 99.83%\n"
     ]
    }
   ],
   "source": [
    "print_results(dos_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U2R data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "u2r_results = classifiers(X_train_u2r, y_train_u2r,\n",
    "                          X_test_u2r, y_test_u2r,\n",
    "                          ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 97.98%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 99.71%\n",
      "cross validation precision: 96.33%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 97.98%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 99.67%\n",
      "cross validation precision: 97.21%\n"
     ]
    }
   ],
   "source": [
    "print_results(u2r_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "u2r_results_kb = classifiers(X_train_u2r_kb, y_train_u2r,\n",
    "                             X_test_u2r_kb, y_test_u2r,\n",
    "                             ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 97.98%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 99.29%\n",
      "cross validation precision: 86.13%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 98.02%\n",
      "precision: 100.00%\n",
      "cross validation accuracy: 99.20%\n",
      "cross validation precision: 86.62%\n"
     ]
    }
   ],
   "source": [
    "print_results(u2r_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2L data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2l_results = classifiers(X_train_r2l, y_train_r2l,\n",
    "                          X_test_r2l, y_test_r2l,\n",
    "                          ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 78.56%\n",
      "precision: 100.00%\n",
      "cross validation accuracy: 98.06%\n",
      "cross validation precision: 95.79%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 77.90%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 97.87%\n",
      "cross validation precision: 96.40%\n"
     ]
    }
   ],
   "source": [
    "print_results(r2l_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "r2l_results_kb = classifiers(X_train_r2l_kb, y_train_r2l,\n",
    "                             X_test_r2l_kb, y_test_r2l,\n",
    "                             ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 79.67%\n",
      "precision: 100.00%\n",
      "cross validation accuracy: 97.90%\n",
      "cross validation precision: 95.83%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 77.91%\n",
      "precision: 0.00%\n",
      "cross validation accuracy: 98.03%\n",
      "cross validation precision: 96.70%\n"
     ]
    }
   ],
   "source": [
    "print_results(r2l_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All data binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_bin_results = classifiers(X_train, y_train2,\n",
    "                                    X_test, y_test2,\n",
    "                                    ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 74.61%\n",
      "precision: 97.15%\n",
      "cross validation accuracy: 98.54%\n",
      "cross validation precision: 98.77%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 75.18%\n",
      "precision: 96.87%\n",
      "cross validation accuracy: 98.54%\n",
      "cross validation precision: 98.80%\n"
     ]
    }
   ],
   "source": [
    "print_results(all_class_bin_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_bin_results_kb = classifiers(X_train_kb2, y_train2,\n",
    "                                       X_test_kb2, y_test2,\n",
    "                                       ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 76.80%\n",
      "precision: 96.79%\n",
      "cross validation accuracy: 98.35%\n",
      "cross validation precision: 98.65%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 76.91%\n",
      "precision: 97.03%\n",
      "cross validation accuracy: 98.27%\n",
      "cross validation precision: 98.69%\n"
     ]
    }
   ],
   "source": [
    "print_results(all_class_bin_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All data multi classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "multi_class_results = classifiers_macro(X_train, y_train5,\n",
    "                                        X_test, y_test5,\n",
    "                                        ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 70.72%\n",
      "cross validation accuracy: 98.42%\n",
      "precision: 68.60%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 72.48%\n",
      "cross validation accuracy: 98.24%\n",
      "precision: 48.89%\n"
     ]
    }
   ],
   "source": [
    "print_results(multi_class_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- -\n",
    "- Results with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shintay/.pyenv/versions/3.6.7/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "multi_class_results_kb = classifiers_macro(X_train_kb5, y_train5,\n",
    "                                           X_test_kb5, y_test5,\n",
    "                                           ensemble_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Adaboost:\n",
      " \u001b[0;0m\n",
      "accuracy: 73.62%\n",
      "cross validation accuracy: 98.23%\n",
      "precision: 89.20%\n",
      "\u001b[1m \n",
      "Random Forest:\n",
      " \u001b[0;0m\n",
      "accuracy: 72.37%\n",
      "cross validation accuracy: 98.06%\n",
      "precision: 68.73%\n"
     ]
    }
   ],
   "source": [
    "print_results(multi_class_results_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ANN Model\n",
    "<br> Use of deep neural networks for classification purposes. \n",
    "<br> Both scenarios are handled, \n",
    "- 'Normal' / 'Attack' binary classification; \n",
    "-  The more complex 5 types of traffic, multi classification one. \n",
    "<br><br> The 'segmented data set' approaches are ignored for ANN model, \n",
    "<br>as it can handle all traffic together with very good accuracy scores,\n",
    "<br>even with just 20 selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing data for neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data pre-processing\n",
    "train_target2 = df_train_01['class_2'] \n",
    "train_target5 = df_train_01['class_5']\n",
    "df_train_02.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "y_train2 = train_target2.astype('category').cat.codes\n",
    "y_train5 = train_target5.astype('category').cat.codes\n",
    "\n",
    "# Test data pre-processing\n",
    "test_target2 = df_test_01['class_2'] \n",
    "test_target5 = df_test_01['class_5']\n",
    "df_test_02.drop(['class_2', 'class_5'], axis=1, inplace=True)\n",
    "y_test2 = test_target2.astype('category').cat.codes\n",
    "y_test5 = test_target5.astype('category').cat.codes\n",
    "\n",
    "# convert labels to NN multiclass\n",
    "y_train5 = np_utils.to_categorical(y_train5, 5)\n",
    "y_test5 = np_utils.to_categorical(y_test5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural network definitions for **binary classification**\n",
    "- Using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125973 samples, validate on 22544 samples\n",
      "Epoch 1/100\n",
      "125973/125973 [==============================] - 10s 77us/step - loss: 0.0868 - acc: 0.9679 - val_loss: 1.2441 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.01047\n",
      "Epoch 2/100\n",
      "125973/125973 [==============================] - 9s 68us/step - loss: 0.0519 - acc: 0.9825 - val_loss: 1.0400 - val_acc: 0.7820\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.01047\n",
      "Epoch 3/100\n",
      "125973/125973 [==============================] - 9s 69us/step - loss: 0.0411 - acc: 0.9863 - val_loss: 1.3822 - val_acc: 0.7536\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.01047\n",
      "Epoch 4/100\n",
      "125973/125973 [==============================] - 10s 76us/step - loss: 0.0350 - acc: 0.9889 - val_loss: 1.5576 - val_acc: 0.7537\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.01047\n",
      "Epoch 5/100\n",
      "125973/125973 [==============================] - 10s 76us/step - loss: 0.0306 - acc: 0.9904 - val_loss: 1.3925 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.01047\n",
      "Epoch 6/100\n",
      "125973/125973 [==============================] - 10s 76us/step - loss: 0.0278 - acc: 0.9913 - val_loss: 0.8888 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00006: loss did not improve from 0.01047\n",
      "Epoch 7/100\n",
      "125973/125973 [==============================] - 10s 83us/step - loss: 0.0249 - acc: 0.9925 - val_loss: 1.0410 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.01047\n",
      "Epoch 8/100\n",
      "125973/125973 [==============================] - 9s 75us/step - loss: 0.0245 - acc: 0.9926 - val_loss: 0.9524 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.01047\n",
      "Epoch 9/100\n",
      "125973/125973 [==============================] - 10s 78us/step - loss: 0.0227 - acc: 0.9930 - val_loss: 1.2729 - val_acc: 0.7618\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.01047\n",
      "Epoch 10/100\n",
      "125973/125973 [==============================] - 10s 80us/step - loss: 0.0217 - acc: 0.9936 - val_loss: 1.3109 - val_acc: 0.7717\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.01047\n",
      "Epoch 11/100\n",
      "125973/125973 [==============================] - 9s 69us/step - loss: 0.0204 - acc: 0.9938 - val_loss: 1.3798 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.01047\n",
      "Epoch 12/100\n",
      "125973/125973 [==============================] - 9s 69us/step - loss: 0.0199 - acc: 0.9941 - val_loss: 1.4500 - val_acc: 0.7876\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.01047\n",
      "Epoch 13/100\n",
      "125973/125973 [==============================] - 8s 66us/step - loss: 0.0191 - acc: 0.9945 - val_loss: 0.9221 - val_acc: 0.7842\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.01047\n",
      "Epoch 14/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0182 - acc: 0.9946 - val_loss: 1.4426 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.01047\n",
      "Epoch 15/100\n",
      "125973/125973 [==============================] - 9s 73us/step - loss: 0.0180 - acc: 0.9946 - val_loss: 1.2772 - val_acc: 0.7759\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.01047\n",
      "Epoch 16/100\n",
      "125973/125973 [==============================] - 9s 74us/step - loss: 0.0173 - acc: 0.9949 - val_loss: 1.4147 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.01047\n",
      "Epoch 17/100\n",
      "125973/125973 [==============================] - 10s 76us/step - loss: 0.0169 - acc: 0.9951 - val_loss: 1.6740 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.01047\n",
      "Epoch 18/100\n",
      "125973/125973 [==============================] - 10s 77us/step - loss: 0.0159 - acc: 0.9952 - val_loss: 1.6048 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.01047\n",
      "Epoch 19/100\n",
      "125973/125973 [==============================] - 10s 78us/step - loss: 0.0161 - acc: 0.9952 - val_loss: 1.3164 - val_acc: 0.7740\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.01047\n",
      "Epoch 20/100\n",
      "125973/125973 [==============================] - 10s 78us/step - loss: 0.0154 - acc: 0.9956 - val_loss: 1.3037 - val_acc: 0.7863\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.01047\n",
      "Epoch 21/100\n",
      "125973/125973 [==============================] - 10s 80us/step - loss: 0.0151 - acc: 0.9956 - val_loss: 1.8108 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.01047\n",
      "Epoch 22/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0150 - acc: 0.9955 - val_loss: 1.0567 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.01047\n",
      "Epoch 23/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0147 - acc: 0.9957 - val_loss: 1.2672 - val_acc: 0.8053\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.01047\n",
      "Epoch 24/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0145 - acc: 0.9957 - val_loss: 1.3715 - val_acc: 0.7934\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.01047\n",
      "Epoch 25/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0144 - acc: 0.9957 - val_loss: 1.2967 - val_acc: 0.7929\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.01047\n",
      "Epoch 26/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0138 - acc: 0.9958 - val_loss: 1.2935 - val_acc: 0.7989\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.01047\n",
      "Epoch 27/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0133 - acc: 0.9961 - val_loss: 1.5486 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.01047\n",
      "Epoch 28/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0136 - acc: 0.9960 - val_loss: 1.0805 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.01047\n",
      "Epoch 29/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0134 - acc: 0.9960 - val_loss: 1.7406 - val_acc: 0.7928\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.01047\n",
      "Epoch 30/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0132 - acc: 0.9960 - val_loss: 1.3798 - val_acc: 0.7894\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.01047\n",
      "Epoch 31/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0130 - acc: 0.9960 - val_loss: 1.2831 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.01047\n",
      "Epoch 32/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0133 - acc: 0.9962 - val_loss: 1.5042 - val_acc: 0.7883\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.01047\n",
      "Epoch 33/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0131 - acc: 0.9959 - val_loss: 1.5102 - val_acc: 0.7966\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.01047\n",
      "Epoch 34/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0126 - acc: 0.9960 - val_loss: 1.7527 - val_acc: 0.7853\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.01047\n",
      "Epoch 35/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0127 - acc: 0.9960 - val_loss: 1.5073 - val_acc: 0.7966\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.01047\n",
      "Epoch 36/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0116 - acc: 0.9965 - val_loss: 1.7090 - val_acc: 0.7826\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.01047\n",
      "Epoch 37/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0118 - acc: 0.9964 - val_loss: 1.4339 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.01047\n",
      "Epoch 38/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0118 - acc: 0.9963 - val_loss: 1.4955 - val_acc: 0.7935\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.01047\n",
      "Epoch 39/100\n",
      "125973/125973 [==============================] - 8s 65us/step - loss: 0.0112 - acc: 0.9966 - val_loss: 1.5547 - val_acc: 0.7924\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.01047\n",
      "Epoch 40/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0116 - acc: 0.9965 - val_loss: 1.5714 - val_acc: 0.7927\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.01047\n",
      "Epoch 41/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0113 - acc: 0.9964 - val_loss: 1.6242 - val_acc: 0.7992\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.01047\n",
      "Epoch 42/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0113 - acc: 0.9966 - val_loss: 1.7356 - val_acc: 0.7889\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.01047\n",
      "Epoch 43/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 1.3943 - val_acc: 0.8131\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.01047\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0104 - acc: 0.9966 - val_loss: 1.6342 - val_acc: 0.7866\n",
      "\n",
      "Epoch 00044: loss improved from 0.01047 to 0.01039, saving model to dnn/checkpoints/checkpoint_2-44.hdf5\n",
      "Epoch 45/100\n",
      "125973/125973 [==============================] - 8s 66us/step - loss: 0.0108 - acc: 0.9968 - val_loss: 1.7250 - val_acc: 0.8049\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.01039\n",
      "Epoch 46/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0109 - acc: 0.9966 - val_loss: 1.5883 - val_acc: 0.7888\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.01039\n",
      "Epoch 47/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0105 - acc: 0.9968 - val_loss: 1.7783 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.01039\n",
      "Epoch 48/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0099 - acc: 0.9969 - val_loss: 1.7184 - val_acc: 0.7924\n",
      "\n",
      "Epoch 00048: loss improved from 0.01039 to 0.00992, saving model to dnn/checkpoints/checkpoint_2-48.hdf5\n",
      "Epoch 49/100\n",
      "125973/125973 [==============================] - 8s 65us/step - loss: 0.0102 - acc: 0.9968 - val_loss: 1.8139 - val_acc: 0.7792\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00992\n",
      "Epoch 50/100\n",
      "125973/125973 [==============================] - 8s 65us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 1.9108 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00050: loss improved from 0.00992 to 0.00959, saving model to dnn/checkpoints/checkpoint_2-50.hdf5\n",
      "Epoch 51/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0101 - acc: 0.9967 - val_loss: 1.7572 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.00959\n",
      "Epoch 52/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 1.7530 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00052: loss improved from 0.00959 to 0.00941, saving model to dnn/checkpoints/checkpoint_2-52.hdf5\n",
      "Epoch 53/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0096 - acc: 0.9969 - val_loss: 1.7493 - val_acc: 0.7849\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.00941\n",
      "Epoch 54/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 1.6018 - val_acc: 0.8129\n",
      "\n",
      "Epoch 00054: loss improved from 0.00941 to 0.00937, saving model to dnn/checkpoints/checkpoint_2-54.hdf5\n",
      "Epoch 55/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 1.8565 - val_acc: 0.7877\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.00937\n",
      "Epoch 56/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 1.6676 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.00937\n",
      "Epoch 57/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0094 - acc: 0.9972 - val_loss: 1.6251 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00057: loss improved from 0.00937 to 0.00935, saving model to dnn/checkpoints/checkpoint_2-57.hdf5\n",
      "Epoch 58/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 1.3717 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00058: loss improved from 0.00935 to 0.00912, saving model to dnn/checkpoints/checkpoint_2-58.hdf5\n",
      "Epoch 59/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0091 - acc: 0.9973 - val_loss: 1.9132 - val_acc: 0.8123\n",
      "\n",
      "Epoch 00059: loss improved from 0.00912 to 0.00910, saving model to dnn/checkpoints/checkpoint_2-59.hdf5\n",
      "Epoch 60/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 1.4930 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.00910\n",
      "Epoch 61/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0089 - acc: 0.9972 - val_loss: 1.7823 - val_acc: 0.8104\n",
      "\n",
      "Epoch 00061: loss improved from 0.00910 to 0.00887, saving model to dnn/checkpoints/checkpoint_2-61.hdf5\n",
      "Epoch 62/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0091 - acc: 0.9973 - val_loss: 1.7335 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.00887\n",
      "Epoch 63/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0094 - acc: 0.9971 - val_loss: 1.5461 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.00887\n",
      "Epoch 64/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0089 - acc: 0.9971 - val_loss: 1.6632 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.00887\n",
      "Epoch 65/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0090 - acc: 0.9972 - val_loss: 1.6968 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.00887\n",
      "Epoch 66/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0087 - acc: 0.9973 - val_loss: 1.8243 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00066: loss improved from 0.00887 to 0.00874, saving model to dnn/checkpoints/checkpoint_2-66.hdf5\n",
      "Epoch 67/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0087 - acc: 0.9974 - val_loss: 1.7535 - val_acc: 0.8024\n",
      "\n",
      "Epoch 00067: loss improved from 0.00874 to 0.00870, saving model to dnn/checkpoints/checkpoint_2-67.hdf5\n",
      "Epoch 68/100\n",
      "125973/125973 [==============================] - 8s 65us/step - loss: 0.0091 - acc: 0.9972 - val_loss: 1.7956 - val_acc: 0.8046\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.00870\n",
      "Epoch 69/100\n",
      "125973/125973 [==============================] - 8s 62us/step - loss: 0.0087 - acc: 0.9973 - val_loss: 1.9689 - val_acc: 0.7866\n",
      "\n",
      "Epoch 00069: loss improved from 0.00870 to 0.00866, saving model to dnn/checkpoints/checkpoint_2-69.hdf5\n",
      "Epoch 70/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0092 - acc: 0.9972 - val_loss: 1.8899 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.00866\n",
      "Epoch 71/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 1.7288 - val_acc: 0.8133\n",
      "\n",
      "Epoch 00071: loss improved from 0.00866 to 0.00832, saving model to dnn/checkpoints/checkpoint_2-71.hdf5\n",
      "Epoch 72/100\n",
      "125973/125973 [==============================] - 8s 60us/step - loss: 0.0087 - acc: 0.9973 - val_loss: 2.0926 - val_acc: 0.7759\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.00832\n",
      "Epoch 73/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0084 - acc: 0.9974 - val_loss: 2.0057 - val_acc: 0.7866\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.00832\n",
      "Epoch 74/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0086 - acc: 0.9974 - val_loss: 1.8133 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.00832\n",
      "Epoch 75/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 1.6581 - val_acc: 0.8133\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.00832\n",
      "Epoch 76/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0080 - acc: 0.9975 - val_loss: 1.9171 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00076: loss improved from 0.00832 to 0.00804, saving model to dnn/checkpoints/checkpoint_2-76.hdf5\n",
      "Epoch 77/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0085 - acc: 0.9973 - val_loss: 1.7464 - val_acc: 0.7808\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.00804\n",
      "Epoch 78/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0079 - acc: 0.9974 - val_loss: 1.6402 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00078: loss improved from 0.00804 to 0.00793, saving model to dnn/checkpoints/checkpoint_2-78.hdf5\n",
      "Epoch 79/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0092 - acc: 0.9971 - val_loss: 1.9625 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.00793\n",
      "Epoch 80/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 2.0054 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.00793\n",
      "Epoch 81/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 1.8313 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.00793\n",
      "Epoch 82/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0087 - acc: 0.9973 - val_loss: 1.9084 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.00793\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0082 - acc: 0.9974 - val_loss: 2.1032 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.00793\n",
      "Epoch 84/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0084 - acc: 0.9975 - val_loss: 1.7206 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.00793\n",
      "Epoch 85/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0082 - acc: 0.9975 - val_loss: 1.8156 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.00793\n",
      "Epoch 86/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 2.0680 - val_acc: 0.7854\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.00793\n",
      "Epoch 87/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0084 - acc: 0.9975 - val_loss: 1.8470 - val_acc: 0.7931\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.00793\n",
      "Epoch 88/100\n",
      "125973/125973 [==============================] - 7s 59us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 1.7305 - val_acc: 0.8132\n",
      "\n",
      "Epoch 00088: loss improved from 0.00793 to 0.00778, saving model to dnn/checkpoints/checkpoint_2-88.hdf5\n",
      "Epoch 89/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 1.7889 - val_acc: 0.8111\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.00778\n",
      "Epoch 90/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0075 - acc: 0.9977 - val_loss: 2.1690 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00090: loss improved from 0.00778 to 0.00748, saving model to dnn/checkpoints/checkpoint_2-90.hdf5\n",
      "Epoch 91/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 2.0319 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.00748\n",
      "Epoch 92/100\n",
      "125973/125973 [==============================] - 8s 62us/step - loss: 0.0079 - acc: 0.9974 - val_loss: 1.9518 - val_acc: 0.8035\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.00748\n",
      "Epoch 93/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0081 - acc: 0.9975 - val_loss: 2.2378 - val_acc: 0.7856\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.00748\n",
      "Epoch 94/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 1.8210 - val_acc: 0.7816\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.00748\n",
      "Epoch 95/100\n",
      "125973/125973 [==============================] - 8s 62us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 1.9340 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.00748\n",
      "Epoch 96/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 2.1410 - val_acc: 0.7957\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.00748\n",
      "Epoch 97/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0077 - acc: 0.9977 - val_loss: 2.0235 - val_acc: 0.7906\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.00748\n",
      "Epoch 98/100\n",
      "125973/125973 [==============================] - 8s 62us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 1.8299 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00098: loss improved from 0.00748 to 0.00737, saving model to dnn/checkpoints/checkpoint_2-98.hdf5\n",
      "Epoch 99/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 1.9246 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.00737\n",
      "Epoch 100/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0077 - acc: 0.9976 - val_loss: 1.9952 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.00737\n"
     ]
    }
   ],
   "source": [
    "# dnn model for classification \n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# definitions\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(1024, \n",
    "                  input_dim = X_train.shape[1], \n",
    "                  activation = 'relu'))\n",
    "model_2.add(Dropout(0.1))\n",
    "model_2.add(Dense(128, \n",
    "              activation = 'relu')) \n",
    "model_2.add(Dense(1))\n",
    "model_2.add(Activation('sigmoid'))\n",
    "\n",
    "# optimizers and configs\n",
    "model_2.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "csv_logger = CSVLogger('dnn/dnn_2_analysis.csv',\n",
    "                       separator = ',', \n",
    "                       append = False)\n",
    "\n",
    "model_2.fit(X_train, y_train2, \n",
    "          validation_data = (X_test, y_test2),\n",
    "          batch_size = batch_size, \n",
    "          epochs = epochs, \n",
    "          callbacks = [checkpointer, csv_logger])\n",
    "\n",
    "model_2.save('dnn/dnn_model_2.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification:\n",
    "- Results with **best features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125973 samples, validate on 22544 samples\n",
      "Epoch 1/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0949 - acc: 0.9657 - val_loss: 1.0345 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.09490, saving model to dnn/checkpoints/checkpoint_kb2-01.hdf5\n",
      "Epoch 2/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0605 - acc: 0.9783 - val_loss: 1.1798 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00002: loss improved from 0.09490 to 0.06047, saving model to dnn/checkpoints/checkpoint_kb2-02.hdf5\n",
      "Epoch 3/100\n",
      "125973/125973 [==============================] - 5s 38us/step - loss: 0.0508 - acc: 0.9822 - val_loss: 1.2412 - val_acc: 0.7613\n",
      "\n",
      "Epoch 00003: loss improved from 0.06047 to 0.05080, saving model to dnn/checkpoints/checkpoint_kb2-03.hdf5\n",
      "Epoch 4/100\n",
      "125973/125973 [==============================] - 5s 38us/step - loss: 0.0451 - acc: 0.9846 - val_loss: 1.2681 - val_acc: 0.7563\n",
      "\n",
      "Epoch 00004: loss improved from 0.05080 to 0.04507, saving model to dnn/checkpoints/checkpoint_kb2-04.hdf5\n",
      "Epoch 5/100\n",
      "125973/125973 [==============================] - 5s 41us/step - loss: 0.0396 - acc: 0.9869 - val_loss: 1.0557 - val_acc: 0.7722\n",
      "\n",
      "Epoch 00005: loss improved from 0.04507 to 0.03960, saving model to dnn/checkpoints/checkpoint_kb2-05.hdf5\n",
      "Epoch 6/100\n",
      "125973/125973 [==============================] - 5s 38us/step - loss: 0.0365 - acc: 0.9879 - val_loss: 1.0838 - val_acc: 0.7607\n",
      "\n",
      "Epoch 00006: loss improved from 0.03960 to 0.03654, saving model to dnn/checkpoints/checkpoint_kb2-06.hdf5\n",
      "Epoch 7/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0329 - acc: 0.9891 - val_loss: 1.3194 - val_acc: 0.7504\n",
      "\n",
      "Epoch 00007: loss improved from 0.03654 to 0.03293, saving model to dnn/checkpoints/checkpoint_kb2-07.hdf5\n",
      "Epoch 8/100\n",
      "125973/125973 [==============================] - 4s 35us/step - loss: 0.0315 - acc: 0.9898 - val_loss: 1.1111 - val_acc: 0.7665\n",
      "\n",
      "Epoch 00008: loss improved from 0.03293 to 0.03150, saving model to dnn/checkpoints/checkpoint_kb2-08.hdf5\n",
      "Epoch 9/100\n",
      "125973/125973 [==============================] - 4s 32us/step - loss: 0.0298 - acc: 0.9903 - val_loss: 1.0241 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00009: loss improved from 0.03150 to 0.02980, saving model to dnn/checkpoints/checkpoint_kb2-09.hdf5\n",
      "Epoch 10/100\n",
      "125973/125973 [==============================] - 4s 31us/step - loss: 0.0281 - acc: 0.9910 - val_loss: 1.2518 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00010: loss improved from 0.02980 to 0.02811, saving model to dnn/checkpoints/checkpoint_kb2-10.hdf5\n",
      "Epoch 11/100\n",
      "125973/125973 [==============================] - 4s 32us/step - loss: 0.0269 - acc: 0.9912 - val_loss: 1.2009 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00011: loss improved from 0.02811 to 0.02691, saving model to dnn/checkpoints/checkpoint_kb2-11.hdf5\n",
      "Epoch 12/100\n",
      "125973/125973 [==============================] - 4s 33us/step - loss: 0.0260 - acc: 0.9916 - val_loss: 1.2287 - val_acc: 0.7561\n",
      "\n",
      "Epoch 00012: loss improved from 0.02691 to 0.02600, saving model to dnn/checkpoints/checkpoint_kb2-12.hdf5\n",
      "Epoch 13/100\n",
      "125973/125973 [==============================] - 4s 32us/step - loss: 0.0255 - acc: 0.9918 - val_loss: 1.0117 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00013: loss improved from 0.02600 to 0.02553, saving model to dnn/checkpoints/checkpoint_kb2-13.hdf5\n",
      "Epoch 14/100\n",
      "125973/125973 [==============================] - 4s 33us/step - loss: 0.0242 - acc: 0.9922 - val_loss: 1.2436 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00014: loss improved from 0.02553 to 0.02423, saving model to dnn/checkpoints/checkpoint_kb2-14.hdf5\n",
      "Epoch 15/100\n",
      "125973/125973 [==============================] - 4s 33us/step - loss: 0.0234 - acc: 0.9926 - val_loss: 0.8588 - val_acc: 0.7831\n",
      "\n",
      "Epoch 00015: loss improved from 0.02423 to 0.02342, saving model to dnn/checkpoints/checkpoint_kb2-15.hdf5\n",
      "Epoch 16/100\n",
      "125973/125973 [==============================] - 4s 34us/step - loss: 0.0229 - acc: 0.9929 - val_loss: 1.3055 - val_acc: 0.7641\n",
      "\n",
      "Epoch 00016: loss improved from 0.02342 to 0.02291, saving model to dnn/checkpoints/checkpoint_kb2-16.hdf5\n",
      "Epoch 17/100\n",
      "125973/125973 [==============================] - 4s 34us/step - loss: 0.0222 - acc: 0.9930 - val_loss: 1.2014 - val_acc: 0.7866\n",
      "\n",
      "Epoch 00017: loss improved from 0.02291 to 0.02217, saving model to dnn/checkpoints/checkpoint_kb2-17.hdf5\n",
      "Epoch 18/100\n",
      "125973/125973 [==============================] - 5s 39us/step - loss: 0.0217 - acc: 0.9932 - val_loss: 1.3354 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00018: loss improved from 0.02217 to 0.02169, saving model to dnn/checkpoints/checkpoint_kb2-18.hdf5\n",
      "Epoch 19/100\n",
      "125973/125973 [==============================] - 4s 32us/step - loss: 0.0207 - acc: 0.9937 - val_loss: 1.0503 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00019: loss improved from 0.02169 to 0.02070, saving model to dnn/checkpoints/checkpoint_kb2-19.hdf5\n",
      "Epoch 20/100\n",
      "125973/125973 [==============================] - 4s 35us/step - loss: 0.0198 - acc: 0.9939 - val_loss: 1.5002 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00020: loss improved from 0.02070 to 0.01982, saving model to dnn/checkpoints/checkpoint_kb2-20.hdf5\n",
      "Epoch 21/100\n",
      "125973/125973 [==============================] - 5s 37us/step - loss: 0.0196 - acc: 0.9939 - val_loss: 1.4277 - val_acc: 0.7584\n",
      "\n",
      "Epoch 00021: loss improved from 0.01982 to 0.01965, saving model to dnn/checkpoints/checkpoint_kb2-21.hdf5\n",
      "Epoch 22/100\n",
      "125973/125973 [==============================] - 4s 35us/step - loss: 0.0193 - acc: 0.9942 - val_loss: 1.2696 - val_acc: 0.7711\n",
      "\n",
      "Epoch 00022: loss improved from 0.01965 to 0.01931, saving model to dnn/checkpoints/checkpoint_kb2-22.hdf5\n",
      "Epoch 23/100\n",
      "125973/125973 [==============================] - 4s 35us/step - loss: 0.0185 - acc: 0.9943 - val_loss: 1.4386 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00023: loss improved from 0.01931 to 0.01847, saving model to dnn/checkpoints/checkpoint_kb2-23.hdf5\n",
      "Epoch 24/100\n",
      "125973/125973 [==============================] - 4s 32us/step - loss: 0.0182 - acc: 0.9944 - val_loss: 1.3645 - val_acc: 0.7614\n",
      "\n",
      "Epoch 00024: loss improved from 0.01847 to 0.01818, saving model to dnn/checkpoints/checkpoint_kb2-24.hdf5\n",
      "Epoch 25/100\n",
      "125973/125973 [==============================] - 4s 31us/step - loss: 0.0182 - acc: 0.9945 - val_loss: 1.3855 - val_acc: 0.7818\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.01818\n",
      "Epoch 26/100\n",
      "125973/125973 [==============================] - 4s 30us/step - loss: 0.0176 - acc: 0.9946 - val_loss: 1.5470 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00026: loss improved from 0.01818 to 0.01761, saving model to dnn/checkpoints/checkpoint_kb2-26.hdf5\n",
      "Epoch 27/100\n",
      "125973/125973 [==============================] - 4s 36us/step - loss: 0.0171 - acc: 0.9948 - val_loss: 1.5163 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00027: loss improved from 0.01761 to 0.01711, saving model to dnn/checkpoints/checkpoint_kb2-27.hdf5\n",
      "Epoch 28/100\n",
      "125973/125973 [==============================] - 5s 36us/step - loss: 0.0171 - acc: 0.9947 - val_loss: 1.5215 - val_acc: 0.7836\n",
      "\n",
      "Epoch 00028: loss improved from 0.01711 to 0.01711, saving model to dnn/checkpoints/checkpoint_kb2-28.hdf5\n",
      "Epoch 29/100\n",
      "125973/125973 [==============================] - 4s 32us/step - loss: 0.0167 - acc: 0.9948 - val_loss: 1.4682 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00029: loss improved from 0.01711 to 0.01670, saving model to dnn/checkpoints/checkpoint_kb2-29.hdf5\n",
      "Epoch 30/100\n",
      "125973/125973 [==============================] - 5s 37us/step - loss: 0.0160 - acc: 0.9951 - val_loss: 1.3925 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00030: loss improved from 0.01670 to 0.01596, saving model to dnn/checkpoints/checkpoint_kb2-30.hdf5\n",
      "Epoch 31/100\n",
      "125973/125973 [==============================] - 4s 34us/step - loss: 0.0164 - acc: 0.9950 - val_loss: 1.4160 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.01596\n",
      "Epoch 32/100\n",
      "125973/125973 [==============================] - 4s 33us/step - loss: 0.0162 - acc: 0.9949 - val_loss: 1.1287 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.01596\n",
      "Epoch 33/100\n",
      "125973/125973 [==============================] - 4s 34us/step - loss: 0.0159 - acc: 0.9951 - val_loss: 1.4412 - val_acc: 0.7765\n",
      "\n",
      "Epoch 00033: loss improved from 0.01596 to 0.01593, saving model to dnn/checkpoints/checkpoint_kb2-33.hdf5\n",
      "Epoch 34/100\n",
      "125973/125973 [==============================] - 4s 32us/step - loss: 0.0154 - acc: 0.9952 - val_loss: 1.3611 - val_acc: 0.7727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: loss improved from 0.01593 to 0.01541, saving model to dnn/checkpoints/checkpoint_kb2-34.hdf5\n",
      "Epoch 35/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0147 - acc: 0.9955 - val_loss: 1.6630 - val_acc: 0.7751\n",
      "\n",
      "Epoch 00035: loss improved from 0.01541 to 0.01474, saving model to dnn/checkpoints/checkpoint_kb2-35.hdf5\n",
      "Epoch 36/100\n",
      "125973/125973 [==============================] - 4s 36us/step - loss: 0.0150 - acc: 0.9952 - val_loss: 1.6029 - val_acc: 0.7843\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.01474\n",
      "Epoch 37/100\n",
      "125973/125973 [==============================] - 4s 31us/step - loss: 0.0149 - acc: 0.9954 - val_loss: 1.6431 - val_acc: 0.7881\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.01474\n",
      "Epoch 38/100\n",
      "125973/125973 [==============================] - 4s 30us/step - loss: 0.0146 - acc: 0.9956 - val_loss: 1.4724 - val_acc: 0.7835\n",
      "\n",
      "Epoch 00038: loss improved from 0.01474 to 0.01463, saving model to dnn/checkpoints/checkpoint_kb2-38.hdf5\n",
      "Epoch 39/100\n",
      "125973/125973 [==============================] - 4s 30us/step - loss: 0.0142 - acc: 0.9956 - val_loss: 1.5257 - val_acc: 0.7792\n",
      "\n",
      "Epoch 00039: loss improved from 0.01463 to 0.01422, saving model to dnn/checkpoints/checkpoint_kb2-39.hdf5\n",
      "Epoch 40/100\n",
      "125973/125973 [==============================] - 4s 31us/step - loss: 0.0142 - acc: 0.9956 - val_loss: 1.3719 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.01422\n",
      "Epoch 41/100\n",
      "125973/125973 [==============================] - 4s 29us/step - loss: 0.0136 - acc: 0.9958 - val_loss: 1.5701 - val_acc: 0.7883\n",
      "\n",
      "Epoch 00041: loss improved from 0.01422 to 0.01361, saving model to dnn/checkpoints/checkpoint_kb2-41.hdf5\n",
      "Epoch 42/100\n",
      "125973/125973 [==============================] - 4s 31us/step - loss: 0.0142 - acc: 0.9955 - val_loss: 1.1857 - val_acc: 0.8014\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.01361\n",
      "Epoch 43/100\n",
      "125973/125973 [==============================] - 3s 28us/step - loss: 0.0139 - acc: 0.9957 - val_loss: 1.5585 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.01361\n",
      "Epoch 44/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0133 - acc: 0.9959 - val_loss: 1.5608 - val_acc: 0.7820\n",
      "\n",
      "Epoch 00044: loss improved from 0.01361 to 0.01334, saving model to dnn/checkpoints/checkpoint_kb2-44.hdf5\n",
      "Epoch 45/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0132 - acc: 0.9959 - val_loss: 1.3762 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00045: loss improved from 0.01334 to 0.01322, saving model to dnn/checkpoints/checkpoint_kb2-45.hdf5\n",
      "Epoch 46/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0135 - acc: 0.9958 - val_loss: 1.5382 - val_acc: 0.7878\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.01322\n",
      "Epoch 47/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0129 - acc: 0.9960 - val_loss: 1.2819 - val_acc: 0.7878\n",
      "\n",
      "Epoch 00047: loss improved from 0.01322 to 0.01293, saving model to dnn/checkpoints/checkpoint_kb2-47.hdf5\n",
      "Epoch 48/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0128 - acc: 0.9958 - val_loss: 1.6661 - val_acc: 0.7737\n",
      "\n",
      "Epoch 00048: loss improved from 0.01293 to 0.01284, saving model to dnn/checkpoints/checkpoint_kb2-48.hdf5\n",
      "Epoch 49/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0130 - acc: 0.9961 - val_loss: 1.3550 - val_acc: 0.7893\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.01284\n",
      "Epoch 50/100\n",
      "125973/125973 [==============================] - 4s 29us/step - loss: 0.0124 - acc: 0.9962 - val_loss: 1.4473 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00050: loss improved from 0.01284 to 0.01240, saving model to dnn/checkpoints/checkpoint_kb2-50.hdf5\n",
      "Epoch 51/100\n",
      "125973/125973 [==============================] - 4s 30us/step - loss: 0.0127 - acc: 0.9961 - val_loss: 1.4813 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.01240\n",
      "Epoch 52/100\n",
      "125973/125973 [==============================] - 4s 30us/step - loss: 0.0123 - acc: 0.9962 - val_loss: 1.6338 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00052: loss improved from 0.01240 to 0.01229, saving model to dnn/checkpoints/checkpoint_kb2-52.hdf5\n",
      "Epoch 53/100\n",
      "125973/125973 [==============================] - 4s 29us/step - loss: 0.0128 - acc: 0.9960 - val_loss: 1.5128 - val_acc: 0.7821\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.01229\n",
      "Epoch 54/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0121 - acc: 0.9962 - val_loss: 1.4239 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00054: loss improved from 0.01229 to 0.01211, saving model to dnn/checkpoints/checkpoint_kb2-54.hdf5\n",
      "Epoch 55/100\n",
      "125973/125973 [==============================] - 4s 29us/step - loss: 0.0122 - acc: 0.9960 - val_loss: 1.7017 - val_acc: 0.7845\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.01211\n",
      "Epoch 56/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0119 - acc: 0.9963 - val_loss: 1.6586 - val_acc: 0.7846\n",
      "\n",
      "Epoch 00056: loss improved from 0.01211 to 0.01186, saving model to dnn/checkpoints/checkpoint_kb2-56.hdf5\n",
      "Epoch 57/100\n",
      "125973/125973 [==============================] - 3s 28us/step - loss: 0.0118 - acc: 0.9964 - val_loss: 1.3744 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00057: loss improved from 0.01186 to 0.01182, saving model to dnn/checkpoints/checkpoint_kb2-57.hdf5\n",
      "Epoch 58/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0117 - acc: 0.9965 - val_loss: 1.7125 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00058: loss improved from 0.01182 to 0.01167, saving model to dnn/checkpoints/checkpoint_kb2-58.hdf5\n",
      "Epoch 59/100\n",
      "125973/125973 [==============================] - 4s 30us/step - loss: 0.0118 - acc: 0.9964 - val_loss: 1.7662 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.01167\n",
      "Epoch 60/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0119 - acc: 0.9963 - val_loss: 1.6476 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.01167\n",
      "Epoch 61/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0117 - acc: 0.9964 - val_loss: 1.6916 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.01167\n",
      "Epoch 62/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0114 - acc: 0.9964 - val_loss: 1.7450 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00062: loss improved from 0.01167 to 0.01136, saving model to dnn/checkpoints/checkpoint_kb2-62.hdf5\n",
      "Epoch 63/100\n",
      "125973/125973 [==============================] - 4s 29us/step - loss: 0.0114 - acc: 0.9964 - val_loss: 1.4336 - val_acc: 0.7939\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.01136\n",
      "Epoch 64/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0116 - acc: 0.9964 - val_loss: 1.5798 - val_acc: 0.7934\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.01136\n",
      "Epoch 65/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0121 - acc: 0.9961 - val_loss: 1.6239 - val_acc: 0.7943\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.01136\n",
      "Epoch 66/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0117 - acc: 0.9962 - val_loss: 1.8830 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.01136\n",
      "Epoch 67/100\n",
      "125973/125973 [==============================] - 3s 28us/step - loss: 0.0120 - acc: 0.9962 - val_loss: 1.7805 - val_acc: 0.7866\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.01136\n",
      "Epoch 68/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0116 - acc: 0.9963 - val_loss: 1.5254 - val_acc: 0.7977\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.01136\n",
      "Epoch 69/100\n",
      "125973/125973 [==============================] - 4s 29us/step - loss: 0.0118 - acc: 0.9962 - val_loss: 1.5279 - val_acc: 0.7966\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.01136\n",
      "Epoch 70/100\n",
      "125973/125973 [==============================] - 3s 28us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 1.5179 - val_acc: 0.8038\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.01136\n",
      "Epoch 71/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0117 - acc: 0.9962 - val_loss: 1.5633 - val_acc: 0.7966\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.01136\n",
      "Epoch 72/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 1.7744 - val_acc: 0.7864\n",
      "\n",
      "Epoch 00072: loss improved from 0.01136 to 0.01098, saving model to dnn/checkpoints/checkpoint_kb2-72.hdf5\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973/125973 [==============================] - 4s 29us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 1.8095 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.01098\n",
      "Epoch 74/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0109 - acc: 0.9966 - val_loss: 1.6623 - val_acc: 0.7993\n",
      "\n",
      "Epoch 00074: loss improved from 0.01098 to 0.01090, saving model to dnn/checkpoints/checkpoint_kb2-74.hdf5\n",
      "Epoch 75/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0112 - acc: 0.9964 - val_loss: 1.6071 - val_acc: 0.8014\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.01090\n",
      "Epoch 76/100\n",
      "125973/125973 [==============================] - 4s 29us/step - loss: 0.0104 - acc: 0.9967 - val_loss: 1.8368 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00076: loss improved from 0.01090 to 0.01041, saving model to dnn/checkpoints/checkpoint_kb2-76.hdf5\n",
      "Epoch 77/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0103 - acc: 0.9968 - val_loss: 1.9113 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00077: loss improved from 0.01041 to 0.01025, saving model to dnn/checkpoints/checkpoint_kb2-77.hdf5\n",
      "Epoch 78/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0104 - acc: 0.9968 - val_loss: 1.8477 - val_acc: 0.7890\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.01025\n",
      "Epoch 79/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0111 - acc: 0.9966 - val_loss: 1.8089 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.01025\n",
      "Epoch 80/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0103 - acc: 0.9969 - val_loss: 1.5581 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.01025\n",
      "Epoch 81/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0101 - acc: 0.9968 - val_loss: 1.6682 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00081: loss improved from 0.01025 to 0.01014, saving model to dnn/checkpoints/checkpoint_kb2-81.hdf5\n",
      "Epoch 82/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 1.5986 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.01014\n",
      "Epoch 83/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0101 - acc: 0.9968 - val_loss: 1.9821 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00083: loss improved from 0.01014 to 0.01008, saving model to dnn/checkpoints/checkpoint_kb2-83.hdf5\n",
      "Epoch 84/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 1.4925 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00084: loss improved from 0.01008 to 0.00989, saving model to dnn/checkpoints/checkpoint_kb2-84.hdf5\n",
      "Epoch 85/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0103 - acc: 0.9968 - val_loss: 1.5311 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.00989\n",
      "Epoch 86/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0102 - acc: 0.9968 - val_loss: 1.6985 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.00989\n",
      "Epoch 87/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0104 - acc: 0.9967 - val_loss: 1.5507 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.00989\n",
      "Epoch 88/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 1.9175 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00088: loss improved from 0.00989 to 0.00960, saving model to dnn/checkpoints/checkpoint_kb2-88.hdf5\n",
      "Epoch 89/100\n",
      "125973/125973 [==============================] - 4s 30us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 1.8287 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.00960\n",
      "Epoch 90/100\n",
      "125973/125973 [==============================] - 4s 30us/step - loss: 0.0096 - acc: 0.9969 - val_loss: 1.8994 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.00960\n",
      "Epoch 91/100\n",
      "125973/125973 [==============================] - 3s 28us/step - loss: 0.0101 - acc: 0.9968 - val_loss: 1.4802 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.00960\n",
      "Epoch 92/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0100 - acc: 0.9969 - val_loss: 1.7887 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.00960\n",
      "Epoch 93/100\n",
      "125973/125973 [==============================] - 4s 30us/step - loss: 0.0096 - acc: 0.9972 - val_loss: 1.9061 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00093: loss improved from 0.00960 to 0.00957, saving model to dnn/checkpoints/checkpoint_kb2-93.hdf5\n",
      "Epoch 94/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0101 - acc: 0.9968 - val_loss: 1.5062 - val_acc: 0.7998\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.00957\n",
      "Epoch 95/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0100 - acc: 0.9969 - val_loss: 1.6218 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.00957\n",
      "Epoch 96/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 1.9906 - val_acc: 0.7983\n",
      "\n",
      "Epoch 00096: loss improved from 0.00957 to 0.00951, saving model to dnn/checkpoints/checkpoint_kb2-96.hdf5\n",
      "Epoch 97/100\n",
      "125973/125973 [==============================] - 4s 28us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 1.6396 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.00951\n",
      "Epoch 98/100\n",
      "125973/125973 [==============================] - 4s 29us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 1.8152 - val_acc: 0.7801\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.00951\n",
      "Epoch 99/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0095 - acc: 0.9972 - val_loss: 1.7886 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.00951\n",
      "Epoch 100/100\n",
      "125973/125973 [==============================] - 3s 27us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 1.8542 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00100: loss improved from 0.00951 to 0.00945, saving model to dnn/checkpoints/checkpoint_kb2-100.hdf5\n"
     ]
    }
   ],
   "source": [
    "# definitions\n",
    "model_kb2 = Sequential()\n",
    "model_kb2.add(Dense(512, \n",
    "                    input_dim = X_train_kb2.shape[1], \n",
    "                    activation = 'relu'))  \n",
    "model_kb2.add(Dropout(0.1))\n",
    "model_kb2.add(Dense(128, \n",
    "              activation = 'relu'))  \n",
    "model_kb2.add(Dense(1))\n",
    "model_kb2.add(Activation('sigmoid'))\n",
    "\n",
    "# optimizers and configs\n",
    "model_kb2.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "csv_logger = CSVLogger('dnn/dnn_kb2_analysis.csv',\n",
    "                       separator = ',', \n",
    "                       append = False)\n",
    "\n",
    "model_kb2.fit(X_train_kb2, y_train2, \n",
    "          validation_data = (X_test_kb2, y_test2),\n",
    "          batch_size = batch_size, \n",
    "          epochs = epochs, \n",
    "          callbacks = [checkpointer, csv_logger])\n",
    "\n",
    "model_kb2.save('dnn/dnn_model_kb2.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural network definitions for **multi classification**\n",
    "- Using **all data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125973 samples, validate on 22544 samples\n",
      "Epoch 1/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.1987 - acc: 0.9399 - val_loss: 1.5929 - val_acc: 0.7142\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.19867, saving model to dnn/checkpoints/checkpoint_5-01.hdf5\n",
      "Epoch 2/100\n",
      "125973/125973 [==============================] - 6s 50us/step - loss: 0.0883 - acc: 0.9728 - val_loss: 1.7156 - val_acc: 0.7376\n",
      "\n",
      "Epoch 00002: loss improved from 0.19867 to 0.08827, saving model to dnn/checkpoints/checkpoint_5-02.hdf5\n",
      "Epoch 3/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0641 - acc: 0.9812 - val_loss: 1.7297 - val_acc: 0.7492\n",
      "\n",
      "Epoch 00003: loss improved from 0.08827 to 0.06411, saving model to dnn/checkpoints/checkpoint_5-03.hdf5\n",
      "Epoch 4/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0521 - acc: 0.9853 - val_loss: 1.9256 - val_acc: 0.7409\n",
      "\n",
      "Epoch 00004: loss improved from 0.06411 to 0.05210, saving model to dnn/checkpoints/checkpoint_5-04.hdf5\n",
      "Epoch 5/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0446 - acc: 0.9878 - val_loss: 2.1401 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00005: loss improved from 0.05210 to 0.04464, saving model to dnn/checkpoints/checkpoint_5-05.hdf5\n",
      "Epoch 6/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0392 - acc: 0.9896 - val_loss: 2.0882 - val_acc: 0.7449\n",
      "\n",
      "Epoch 00006: loss improved from 0.04464 to 0.03917, saving model to dnn/checkpoints/checkpoint_5-06.hdf5\n",
      "Epoch 7/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0348 - acc: 0.9906 - val_loss: 2.4390 - val_acc: 0.6998\n",
      "\n",
      "Epoch 00007: loss improved from 0.03917 to 0.03481, saving model to dnn/checkpoints/checkpoint_5-07.hdf5\n",
      "Epoch 8/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0323 - acc: 0.9915 - val_loss: 2.3077 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00008: loss improved from 0.03481 to 0.03234, saving model to dnn/checkpoints/checkpoint_5-08.hdf5\n",
      "Epoch 9/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0296 - acc: 0.9922 - val_loss: 2.5277 - val_acc: 0.7310\n",
      "\n",
      "Epoch 00009: loss improved from 0.03234 to 0.02961, saving model to dnn/checkpoints/checkpoint_5-09.hdf5\n",
      "Epoch 10/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0277 - acc: 0.9925 - val_loss: 2.5424 - val_acc: 0.7336\n",
      "\n",
      "Epoch 00010: loss improved from 0.02961 to 0.02766, saving model to dnn/checkpoints/checkpoint_5-10.hdf5\n",
      "Epoch 11/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0257 - acc: 0.9934 - val_loss: 2.5083 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00011: loss improved from 0.02766 to 0.02573, saving model to dnn/checkpoints/checkpoint_5-11.hdf5\n",
      "Epoch 12/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0244 - acc: 0.9937 - val_loss: 2.5541 - val_acc: 0.7371\n",
      "\n",
      "Epoch 00012: loss improved from 0.02573 to 0.02440, saving model to dnn/checkpoints/checkpoint_5-12.hdf5\n",
      "Epoch 13/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0231 - acc: 0.9941 - val_loss: 2.5484 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00013: loss improved from 0.02440 to 0.02305, saving model to dnn/checkpoints/checkpoint_5-13.hdf5\n",
      "Epoch 14/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0222 - acc: 0.9943 - val_loss: 2.5726 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00014: loss improved from 0.02305 to 0.02221, saving model to dnn/checkpoints/checkpoint_5-14.hdf5\n",
      "Epoch 15/100\n",
      "125973/125973 [==============================] - 7s 53us/step - loss: 0.0212 - acc: 0.9945 - val_loss: 2.7475 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00015: loss improved from 0.02221 to 0.02123, saving model to dnn/checkpoints/checkpoint_5-15.hdf5\n",
      "Epoch 16/100\n",
      "125973/125973 [==============================] - 6s 47us/step - loss: 0.0207 - acc: 0.9948 - val_loss: 2.6292 - val_acc: 0.7390\n",
      "\n",
      "Epoch 00016: loss improved from 0.02123 to 0.02072, saving model to dnn/checkpoints/checkpoint_5-16.hdf5\n",
      "Epoch 17/100\n",
      "125973/125973 [==============================] - 7s 53us/step - loss: 0.0195 - acc: 0.9950 - val_loss: 2.6745 - val_acc: 0.7426\n",
      "\n",
      "Epoch 00017: loss improved from 0.02072 to 0.01953, saving model to dnn/checkpoints/checkpoint_5-17.hdf5\n",
      "Epoch 18/100\n",
      "125973/125973 [==============================] - 6s 48us/step - loss: 0.0187 - acc: 0.9952 - val_loss: 2.6573 - val_acc: 0.7399\n",
      "\n",
      "Epoch 00018: loss improved from 0.01953 to 0.01873, saving model to dnn/checkpoints/checkpoint_5-18.hdf5\n",
      "Epoch 19/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0185 - acc: 0.9953 - val_loss: 2.6808 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00019: loss improved from 0.01873 to 0.01854, saving model to dnn/checkpoints/checkpoint_5-19.hdf5\n",
      "Epoch 20/100\n",
      "125973/125973 [==============================] - 6s 51us/step - loss: 0.0176 - acc: 0.9956 - val_loss: 2.6794 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00020: loss improved from 0.01854 to 0.01757, saving model to dnn/checkpoints/checkpoint_5-20.hdf5\n",
      "Epoch 21/100\n",
      "125973/125973 [==============================] - 7s 52us/step - loss: 0.0173 - acc: 0.9955 - val_loss: 2.6841 - val_acc: 0.7376\n",
      "\n",
      "Epoch 00021: loss improved from 0.01757 to 0.01725, saving model to dnn/checkpoints/checkpoint_5-21.hdf5\n",
      "Epoch 22/100\n",
      "125973/125973 [==============================] - 6s 47us/step - loss: 0.0171 - acc: 0.9956 - val_loss: 2.4526 - val_acc: 0.7509\n",
      "\n",
      "Epoch 00022: loss improved from 0.01725 to 0.01708, saving model to dnn/checkpoints/checkpoint_5-22.hdf5\n",
      "Epoch 23/100\n",
      "125973/125973 [==============================] - 6s 46us/step - loss: 0.0162 - acc: 0.9958 - val_loss: 2.7590 - val_acc: 0.7163\n",
      "\n",
      "Epoch 00023: loss improved from 0.01708 to 0.01625, saving model to dnn/checkpoints/checkpoint_5-23.hdf5\n",
      "Epoch 24/100\n",
      "125973/125973 [==============================] - 6s 46us/step - loss: 0.0160 - acc: 0.9958 - val_loss: 2.7037 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00024: loss improved from 0.01625 to 0.01598, saving model to dnn/checkpoints/checkpoint_5-24.hdf5\n",
      "Epoch 25/100\n",
      "125973/125973 [==============================] - 6s 51us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 2.6656 - val_acc: 0.7443\n",
      "\n",
      "Epoch 00025: loss improved from 0.01598 to 0.01589, saving model to dnn/checkpoints/checkpoint_5-25.hdf5\n",
      "Epoch 26/100\n",
      "125973/125973 [==============================] - 6s 51us/step - loss: 0.0153 - acc: 0.9960 - val_loss: 2.7764 - val_acc: 0.7427\n",
      "\n",
      "Epoch 00026: loss improved from 0.01589 to 0.01532, saving model to dnn/checkpoints/checkpoint_5-26.hdf5\n",
      "Epoch 27/100\n",
      "125973/125973 [==============================] - 6s 48us/step - loss: 0.0154 - acc: 0.9960 - val_loss: 2.6974 - val_acc: 0.7461\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.01532\n",
      "Epoch 28/100\n",
      "125973/125973 [==============================] - 6s 45us/step - loss: 0.0150 - acc: 0.9961 - val_loss: 2.6494 - val_acc: 0.7472\n",
      "\n",
      "Epoch 00028: loss improved from 0.01532 to 0.01496, saving model to dnn/checkpoints/checkpoint_5-28.hdf5\n",
      "Epoch 29/100\n",
      "125973/125973 [==============================] - 6s 44us/step - loss: 0.0149 - acc: 0.9960 - val_loss: 2.6225 - val_acc: 0.7409\n",
      "\n",
      "Epoch 00029: loss improved from 0.01496 to 0.01487, saving model to dnn/checkpoints/checkpoint_5-29.hdf5\n",
      "Epoch 30/100\n",
      "125973/125973 [==============================] - 6s 45us/step - loss: 0.0142 - acc: 0.9963 - val_loss: 2.6329 - val_acc: 0.7447\n",
      "\n",
      "Epoch 00030: loss improved from 0.01487 to 0.01425, saving model to dnn/checkpoints/checkpoint_5-30.hdf5\n",
      "Epoch 31/100\n",
      "125973/125973 [==============================] - 6s 48us/step - loss: 0.0141 - acc: 0.9961 - val_loss: 2.7040 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00031: loss improved from 0.01425 to 0.01406, saving model to dnn/checkpoints/checkpoint_5-31.hdf5\n",
      "Epoch 32/100\n",
      "125973/125973 [==============================] - 7s 53us/step - loss: 0.0140 - acc: 0.9962 - val_loss: 2.8243 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00032: loss improved from 0.01406 to 0.01396, saving model to dnn/checkpoints/checkpoint_5-32.hdf5\n",
      "Epoch 33/100\n",
      "125973/125973 [==============================] - 6s 49us/step - loss: 0.0133 - acc: 0.9966 - val_loss: 2.7505 - val_acc: 0.7428\n",
      "\n",
      "Epoch 00033: loss improved from 0.01396 to 0.01329, saving model to dnn/checkpoints/checkpoint_5-33.hdf5\n",
      "Epoch 34/100\n",
      "125973/125973 [==============================] - 6s 45us/step - loss: 0.0136 - acc: 0.9964 - val_loss: 2.6860 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.01329\n",
      "Epoch 35/100\n",
      "125973/125973 [==============================] - 6s 44us/step - loss: 0.0129 - acc: 0.9965 - val_loss: 2.5806 - val_acc: 0.7506\n",
      "\n",
      "Epoch 00035: loss improved from 0.01329 to 0.01286, saving model to dnn/checkpoints/checkpoint_5-35.hdf5\n",
      "Epoch 36/100\n",
      "125973/125973 [==============================] - 6s 47us/step - loss: 0.0132 - acc: 0.9966 - val_loss: 2.5259 - val_acc: 0.7515\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.01286\n",
      "Epoch 37/100\n",
      "125973/125973 [==============================] - 6s 51us/step - loss: 0.0130 - acc: 0.9964 - val_loss: 2.6796 - val_acc: 0.7483\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.01286\n",
      "Epoch 38/100\n",
      "125973/125973 [==============================] - 7s 53us/step - loss: 0.0127 - acc: 0.9965 - val_loss: 2.5009 - val_acc: 0.7536\n",
      "\n",
      "Epoch 00038: loss improved from 0.01286 to 0.01267, saving model to dnn/checkpoints/checkpoint_5-38.hdf5\n",
      "Epoch 39/100\n",
      "125973/125973 [==============================] - 6s 46us/step - loss: 0.0127 - acc: 0.9966 - val_loss: 2.6885 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.01267\n",
      "Epoch 40/100\n",
      "125973/125973 [==============================] - 6s 48us/step - loss: 0.0124 - acc: 0.9966 - val_loss: 2.5776 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00040: loss improved from 0.01267 to 0.01244, saving model to dnn/checkpoints/checkpoint_5-40.hdf5\n",
      "Epoch 41/100\n",
      "125973/125973 [==============================] - 6s 52us/step - loss: 0.0126 - acc: 0.9966 - val_loss: 2.6391 - val_acc: 0.7503\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.01244\n",
      "Epoch 42/100\n",
      "125973/125973 [==============================] - 6s 45us/step - loss: 0.0120 - acc: 0.9968 - val_loss: 2.7993 - val_acc: 0.7473\n",
      "\n",
      "Epoch 00042: loss improved from 0.01244 to 0.01201, saving model to dnn/checkpoints/checkpoint_5-42.hdf5\n",
      "Epoch 43/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0121 - acc: 0.9968 - val_loss: 2.5185 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.01201\n",
      "Epoch 44/100\n",
      "125973/125973 [==============================] - 5s 43us/step - loss: 0.0122 - acc: 0.9966 - val_loss: 2.6646 - val_acc: 0.7501\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.01201\n",
      "Epoch 45/100\n",
      "125973/125973 [==============================] - 5s 43us/step - loss: 0.0117 - acc: 0.9969 - val_loss: 2.7052 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00045: loss improved from 0.01201 to 0.01174, saving model to dnn/checkpoints/checkpoint_5-45.hdf5\n",
      "Epoch 46/100\n",
      "125973/125973 [==============================] - 5s 43us/step - loss: 0.0115 - acc: 0.9969 - val_loss: 2.8198 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00046: loss improved from 0.01174 to 0.01148, saving model to dnn/checkpoints/checkpoint_5-46.hdf5\n",
      "Epoch 47/100\n",
      "125973/125973 [==============================] - 6s 44us/step - loss: 0.0118 - acc: 0.9968 - val_loss: 2.5321 - val_acc: 0.7536\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.01148\n",
      "Epoch 48/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0112 - acc: 0.9971 - val_loss: 2.6564 - val_acc: 0.7585\n",
      "\n",
      "Epoch 00048: loss improved from 0.01148 to 0.01124, saving model to dnn/checkpoints/checkpoint_5-48.hdf5\n",
      "Epoch 49/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0111 - acc: 0.9969 - val_loss: 2.6040 - val_acc: 0.7590\n",
      "\n",
      "Epoch 00049: loss improved from 0.01124 to 0.01115, saving model to dnn/checkpoints/checkpoint_5-49.hdf5\n",
      "Epoch 50/100\n",
      "125973/125973 [==============================] - 5s 43us/step - loss: 0.0115 - acc: 0.9968 - val_loss: 2.7567 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.01115\n",
      "Epoch 51/100\n",
      "125973/125973 [==============================] - 5s 41us/step - loss: 0.0111 - acc: 0.9969 - val_loss: 2.6372 - val_acc: 0.7532\n",
      "\n",
      "Epoch 00051: loss improved from 0.01115 to 0.01113, saving model to dnn/checkpoints/checkpoint_5-51.hdf5\n",
      "Epoch 52/100\n",
      "125973/125973 [==============================] - 6s 45us/step - loss: 0.0110 - acc: 0.9969 - val_loss: 2.4454 - val_acc: 0.7612\n",
      "\n",
      "Epoch 00052: loss improved from 0.01113 to 0.01095, saving model to dnn/checkpoints/checkpoint_5-52.hdf5\n",
      "Epoch 53/100\n",
      "125973/125973 [==============================] - 6s 45us/step - loss: 0.0112 - acc: 0.9968 - val_loss: 2.6563 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.01095\n",
      "Epoch 54/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 2.7244 - val_acc: 0.7496\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.01095\n",
      "Epoch 55/100\n",
      "125973/125973 [==============================] - 5s 41us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 2.7743 - val_acc: 0.7476\n",
      "\n",
      "Epoch 00055: loss improved from 0.01095 to 0.01063, saving model to dnn/checkpoints/checkpoint_5-55.hdf5\n",
      "Epoch 56/100\n",
      "125973/125973 [==============================] - 6s 44us/step - loss: 0.0109 - acc: 0.9972 - val_loss: 2.7390 - val_acc: 0.7559\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.01063\n",
      "Epoch 57/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0106 - acc: 0.9972 - val_loss: 2.8576 - val_acc: 0.7395\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.01063\n",
      "Epoch 58/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0104 - acc: 0.9973 - val_loss: 2.6257 - val_acc: 0.7553\n",
      "\n",
      "Epoch 00058: loss improved from 0.01063 to 0.01039, saving model to dnn/checkpoints/checkpoint_5-58.hdf5\n",
      "Epoch 59/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0104 - acc: 0.9971 - val_loss: 2.6068 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00059: loss improved from 0.01039 to 0.01036, saving model to dnn/checkpoints/checkpoint_5-59.hdf5\n",
      "Epoch 60/100\n",
      "125973/125973 [==============================] - 5s 43us/step - loss: 0.0106 - acc: 0.9971 - val_loss: 2.8577 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.01036\n",
      "Epoch 61/100\n",
      "125973/125973 [==============================] - 5s 41us/step - loss: 0.0102 - acc: 0.9971 - val_loss: 2.7294 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00061: loss improved from 0.01036 to 0.01017, saving model to dnn/checkpoints/checkpoint_5-61.hdf5\n",
      "Epoch 62/100\n",
      "125973/125973 [==============================] - 5s 43us/step - loss: 0.0101 - acc: 0.9971 - val_loss: 2.5764 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00062: loss improved from 0.01017 to 0.01010, saving model to dnn/checkpoints/checkpoint_5-62.hdf5\n",
      "Epoch 63/100\n",
      "125973/125973 [==============================] - 5s 43us/step - loss: 0.0102 - acc: 0.9973 - val_loss: 2.5584 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.01010\n",
      "Epoch 64/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0100 - acc: 0.9973 - val_loss: 2.8282 - val_acc: 0.7485\n",
      "\n",
      "Epoch 00064: loss improved from 0.01010 to 0.01004, saving model to dnn/checkpoints/checkpoint_5-64.hdf5\n",
      "Epoch 65/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0101 - acc: 0.9973 - val_loss: 2.7184 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.01004\n",
      "Epoch 66/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0099 - acc: 0.9972 - val_loss: 2.5930 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00066: loss improved from 0.01004 to 0.00992, saving model to dnn/checkpoints/checkpoint_5-66.hdf5\n",
      "Epoch 67/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0096 - acc: 0.9974 - val_loss: 2.8164 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00067: loss improved from 0.00992 to 0.00956, saving model to dnn/checkpoints/checkpoint_5-67.hdf5\n",
      "Epoch 68/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0100 - acc: 0.9972 - val_loss: 2.7006 - val_acc: 0.7583\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.00956\n",
      "Epoch 69/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0095 - acc: 0.9974 - val_loss: 2.6513 - val_acc: 0.7631\n",
      "\n",
      "Epoch 00069: loss improved from 0.00956 to 0.00955, saving model to dnn/checkpoints/checkpoint_5-69.hdf5\n",
      "Epoch 70/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0099 - acc: 0.9972 - val_loss: 2.6882 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.00955\n",
      "Epoch 71/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0095 - acc: 0.9974 - val_loss: 2.8343 - val_acc: 0.7579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00071: loss improved from 0.00955 to 0.00954, saving model to dnn/checkpoints/checkpoint_5-71.hdf5\n",
      "Epoch 72/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0097 - acc: 0.9973 - val_loss: 2.8462 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.00954\n",
      "Epoch 73/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0094 - acc: 0.9975 - val_loss: 2.5133 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00073: loss improved from 0.00954 to 0.00941, saving model to dnn/checkpoints/checkpoint_5-73.hdf5\n",
      "Epoch 74/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0093 - acc: 0.9973 - val_loss: 2.6478 - val_acc: 0.7651\n",
      "\n",
      "Epoch 00074: loss improved from 0.00941 to 0.00929, saving model to dnn/checkpoints/checkpoint_5-74.hdf5\n",
      "Epoch 75/100\n",
      "125973/125973 [==============================] - 5s 43us/step - loss: 0.0096 - acc: 0.9973 - val_loss: 2.6956 - val_acc: 0.7603\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.00929\n",
      "Epoch 76/100\n",
      "125973/125973 [==============================] - 6s 44us/step - loss: 0.0092 - acc: 0.9974 - val_loss: 2.7769 - val_acc: 0.7614\n",
      "\n",
      "Epoch 00076: loss improved from 0.00929 to 0.00924, saving model to dnn/checkpoints/checkpoint_5-76.hdf5\n",
      "Epoch 77/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0096 - acc: 0.9973 - val_loss: 2.7842 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.00924\n",
      "Epoch 78/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0091 - acc: 0.9974 - val_loss: 2.6589 - val_acc: 0.7592\n",
      "\n",
      "Epoch 00078: loss improved from 0.00924 to 0.00910, saving model to dnn/checkpoints/checkpoint_5-78.hdf5\n",
      "Epoch 79/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0091 - acc: 0.9975 - val_loss: 2.7348 - val_acc: 0.7619\n",
      "\n",
      "Epoch 00079: loss improved from 0.00910 to 0.00908, saving model to dnn/checkpoints/checkpoint_5-79.hdf5\n",
      "Epoch 80/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0094 - acc: 0.9972 - val_loss: 2.8918 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.00908\n",
      "Epoch 81/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0092 - acc: 0.9975 - val_loss: 2.7146 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.00908\n",
      "Epoch 82/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0091 - acc: 0.9975 - val_loss: 2.6255 - val_acc: 0.7652\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.00908\n",
      "Epoch 83/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0090 - acc: 0.9975 - val_loss: 2.9168 - val_acc: 0.7584\n",
      "\n",
      "Epoch 00083: loss improved from 0.00908 to 0.00899, saving model to dnn/checkpoints/checkpoint_5-83.hdf5\n",
      "Epoch 84/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0090 - acc: 0.9975 - val_loss: 2.6816 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00084: loss improved from 0.00899 to 0.00898, saving model to dnn/checkpoints/checkpoint_5-84.hdf5\n",
      "Epoch 85/100\n",
      "125973/125973 [==============================] - 6s 50us/step - loss: 0.0091 - acc: 0.9974 - val_loss: 2.8847 - val_acc: 0.7581\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.00898\n",
      "Epoch 86/100\n",
      "125973/125973 [==============================] - 6s 47us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 2.6455 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00086: loss improved from 0.00898 to 0.00888, saving model to dnn/checkpoints/checkpoint_5-86.hdf5\n",
      "Epoch 87/100\n",
      "125973/125973 [==============================] - 6s 46us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 2.6152 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00087: loss improved from 0.00888 to 0.00859, saving model to dnn/checkpoints/checkpoint_5-87.hdf5\n",
      "Epoch 88/100\n",
      "125973/125973 [==============================] - 6s 45us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 3.0291 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.00859\n",
      "Epoch 89/100\n",
      "125973/125973 [==============================] - 5s 43us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 2.7863 - val_acc: 0.7629\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.00859\n",
      "Epoch 90/100\n",
      "125973/125973 [==============================] - 5s 41us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 2.8051 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.00859\n",
      "Epoch 91/100\n",
      "125973/125973 [==============================] - 5s 41us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 2.7245 - val_acc: 0.7614\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.00859\n",
      "Epoch 92/100\n",
      "125973/125973 [==============================] - 5s 41us/step - loss: 0.0087 - acc: 0.9975 - val_loss: 2.9832 - val_acc: 0.7397\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.00859\n",
      "Epoch 93/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 2.8648 - val_acc: 0.7572\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.00859\n",
      "Epoch 94/100\n",
      "125973/125973 [==============================] - 5s 40us/step - loss: 0.0088 - acc: 0.9975 - val_loss: 2.8259 - val_acc: 0.7631\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.00859\n",
      "Epoch 95/100\n",
      "125973/125973 [==============================] - 5s 41us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 2.7947 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.00859\n",
      "Epoch 96/100\n",
      "125973/125973 [==============================] - 5s 38us/step - loss: 0.0088 - acc: 0.9975 - val_loss: 2.7661 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.00859\n",
      "Epoch 97/100\n",
      "125973/125973 [==============================] - 5s 37us/step - loss: 0.0086 - acc: 0.9975 - val_loss: 2.7981 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00097: loss improved from 0.00859 to 0.00855, saving model to dnn/checkpoints/checkpoint_5-97.hdf5\n",
      "Epoch 98/100\n",
      "125973/125973 [==============================] - 5s 42us/step - loss: 0.0083 - acc: 0.9977 - val_loss: 2.8078 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00098: loss improved from 0.00855 to 0.00829, saving model to dnn/checkpoints/checkpoint_5-98.hdf5\n",
      "Epoch 99/100\n",
      "125973/125973 [==============================] - 5s 41us/step - loss: 0.0085 - acc: 0.9975 - val_loss: 2.6987 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.00829\n",
      "Epoch 100/100\n",
      "125973/125973 [==============================] - 5s 38us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 2.8590 - val_acc: 0.7598\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.00829\n"
     ]
    }
   ],
   "source": [
    "# definitions\n",
    "model_5 = Sequential()\n",
    "model_5.add(Dense(1024, \n",
    "                  input_dim = X_train.shape[1], \n",
    "                  activation = 'relu'))  \n",
    "model_5.add(Dropout(0.01))\n",
    "model_5.add(Dense(5))\n",
    "model_5.add(Activation('softmax'))\n",
    "\n",
    "# optimizers and configs\n",
    "model_5.compile(loss = 'categorical_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "csv_logger = CSVLogger('dnn/dnn_5_analysis.csv',\n",
    "                       separator = ',', \n",
    "                       append = False)\n",
    "\n",
    "model_5.fit(X_train, y_train5, \n",
    "            validation_data = (X_test, y_test5),\n",
    "            batch_size = batch_size, \n",
    "            epochs = epochs, \n",
    "            callbacks = [checkpointer, csv_logger])\n",
    "\n",
    "model_5.save('dnn/dnn_model_5.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi classification:**\n",
    "- Results using only the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125973 samples, validate on 22544 samples\n",
      "Epoch 1/100\n",
      "125973/125973 [==============================] - 8s 65us/step - loss: 0.1574 - acc: 0.9504 - val_loss: 1.5444 - val_acc: 0.7306\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.00945\n",
      "Epoch 2/100\n",
      "125973/125973 [==============================] - 8s 60us/step - loss: 0.0783 - acc: 0.9745 - val_loss: 1.8700 - val_acc: 0.7349\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.00945\n",
      "Epoch 3/100\n",
      "125973/125973 [==============================] - 8s 64us/step - loss: 0.0613 - acc: 0.9806 - val_loss: 1.8849 - val_acc: 0.7350\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.00945\n",
      "Epoch 4/100\n",
      "125973/125973 [==============================] - 8s 66us/step - loss: 0.0513 - acc: 0.9847 - val_loss: 1.7580 - val_acc: 0.7180\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.00945\n",
      "Epoch 5/100\n",
      "125973/125973 [==============================] - 7s 59us/step - loss: 0.0446 - acc: 0.9869 - val_loss: 2.0497 - val_acc: 0.7362\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.00945\n",
      "Epoch 6/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0388 - acc: 0.9891 - val_loss: 1.8066 - val_acc: 0.7392\n",
      "\n",
      "Epoch 00006: loss did not improve from 0.00945\n",
      "Epoch 7/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0356 - acc: 0.9899 - val_loss: 2.1442 - val_acc: 0.7415\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.00945\n",
      "Epoch 8/100\n",
      "125973/125973 [==============================] - 8s 60us/step - loss: 0.0329 - acc: 0.9908 - val_loss: 2.2642 - val_acc: 0.7352\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.00945\n",
      "Epoch 9/100\n",
      "125973/125973 [==============================] - 8s 62us/step - loss: 0.0312 - acc: 0.9911 - val_loss: 2.1665 - val_acc: 0.7410\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.00945\n",
      "Epoch 10/100\n",
      "125973/125973 [==============================] - 8s 62us/step - loss: 0.0294 - acc: 0.9919 - val_loss: 2.4962 - val_acc: 0.7395\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.00945\n",
      "Epoch 11/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0277 - acc: 0.9924 - val_loss: 2.2984 - val_acc: 0.7371\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.00945\n",
      "Epoch 12/100\n",
      "125973/125973 [==============================] - 8s 66us/step - loss: 0.0270 - acc: 0.9924 - val_loss: 2.1570 - val_acc: 0.7430\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.00945\n",
      "Epoch 13/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0257 - acc: 0.9928 - val_loss: 2.3733 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.00945\n",
      "Epoch 14/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0250 - acc: 0.9928 - val_loss: 2.2512 - val_acc: 0.7382\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.00945\n",
      "Epoch 15/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0238 - acc: 0.9933 - val_loss: 2.5697 - val_acc: 0.7329\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.00945\n",
      "Epoch 16/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0233 - acc: 0.9932 - val_loss: 2.4864 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.00945\n",
      "Epoch 17/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0217 - acc: 0.9938 - val_loss: 2.4838 - val_acc: 0.7435\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.00945\n",
      "Epoch 18/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0216 - acc: 0.9939 - val_loss: 2.6000 - val_acc: 0.7379\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.00945\n",
      "Epoch 19/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0214 - acc: 0.9939 - val_loss: 2.6254 - val_acc: 0.7330\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.00945\n",
      "Epoch 20/100\n",
      "125973/125973 [==============================] - 6s 51us/step - loss: 0.0217 - acc: 0.9940 - val_loss: 2.5718 - val_acc: 0.7422\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.00945\n",
      "Epoch 21/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0198 - acc: 0.9944 - val_loss: 2.6796 - val_acc: 0.7369\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.00945\n",
      "Epoch 22/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0198 - acc: 0.9943 - val_loss: 2.7586 - val_acc: 0.7467\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.00945\n",
      "Epoch 23/100\n",
      "125973/125973 [==============================] - 7s 52us/step - loss: 0.0189 - acc: 0.9946 - val_loss: 2.4871 - val_acc: 0.7431\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.00945\n",
      "Epoch 24/100\n",
      "125973/125973 [==============================] - 7s 52us/step - loss: 0.0191 - acc: 0.9944 - val_loss: 2.8395 - val_acc: 0.7432\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.00945\n",
      "Epoch 25/100\n",
      "125973/125973 [==============================] - 7s 53us/step - loss: 0.0188 - acc: 0.9946 - val_loss: 2.5203 - val_acc: 0.7393\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.00945\n",
      "Epoch 26/100\n",
      "125973/125973 [==============================] - 7s 53us/step - loss: 0.0180 - acc: 0.9948 - val_loss: 2.6284 - val_acc: 0.7340\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.00945\n",
      "Epoch 27/100\n",
      "125973/125973 [==============================] - 6s 50us/step - loss: 0.0180 - acc: 0.9949 - val_loss: 2.5920 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.00945\n",
      "Epoch 28/100\n",
      "125973/125973 [==============================] - 6s 50us/step - loss: 0.0176 - acc: 0.9950 - val_loss: 2.7968 - val_acc: 0.7414\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.00945\n",
      "Epoch 29/100\n",
      "125973/125973 [==============================] - 6s 48us/step - loss: 0.0172 - acc: 0.9949 - val_loss: 2.7580 - val_acc: 0.7423\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.00945\n",
      "Epoch 30/100\n",
      "125973/125973 [==============================] - 7s 52us/step - loss: 0.0167 - acc: 0.9952 - val_loss: 2.7886 - val_acc: 0.7508\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.00945\n",
      "Epoch 31/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0165 - acc: 0.9955 - val_loss: 2.8910 - val_acc: 0.7366\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.00945\n",
      "Epoch 32/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0165 - acc: 0.9953 - val_loss: 2.7106 - val_acc: 0.7454\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.00945\n",
      "Epoch 33/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 2.6993 - val_acc: 0.7416\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.00945\n",
      "Epoch 34/100\n",
      "125973/125973 [==============================] - 8s 60us/step - loss: 0.0161 - acc: 0.9955 - val_loss: 2.6619 - val_acc: 0.7553\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.00945\n",
      "Epoch 35/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0158 - acc: 0.9955 - val_loss: 2.6703 - val_acc: 0.7391\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.00945\n",
      "Epoch 36/100\n",
      "125973/125973 [==============================] - 8s 60us/step - loss: 0.0157 - acc: 0.9956 - val_loss: 2.8558 - val_acc: 0.7454\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.00945\n",
      "Epoch 37/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0155 - acc: 0.9955 - val_loss: 2.6941 - val_acc: 0.7536\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.00945\n",
      "Epoch 38/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0147 - acc: 0.9958 - val_loss: 2.8759 - val_acc: 0.7582\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.00945\n",
      "Epoch 39/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0158 - acc: 0.9954 - val_loss: 2.8160 - val_acc: 0.7526\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.00945\n",
      "Epoch 40/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0151 - acc: 0.9958 - val_loss: 2.8350 - val_acc: 0.7459\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.00945\n",
      "Epoch 41/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0157 - acc: 0.9954 - val_loss: 2.9797 - val_acc: 0.7441\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.00945\n",
      "Epoch 42/100\n",
      "125973/125973 [==============================] - 7s 59us/step - loss: 0.0143 - acc: 0.9960 - val_loss: 2.8957 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.00945\n",
      "Epoch 43/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0149 - acc: 0.9959 - val_loss: 2.9265 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.00945\n",
      "Epoch 44/100\n",
      "125973/125973 [==============================] - 6s 51us/step - loss: 0.0147 - acc: 0.9958 - val_loss: 2.8464 - val_acc: 0.7491\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.00945\n",
      "Epoch 45/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0145 - acc: 0.9959 - val_loss: 2.7015 - val_acc: 0.7492\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00945\n",
      "Epoch 46/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0140 - acc: 0.9960 - val_loss: 2.8976 - val_acc: 0.7392\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.00945\n",
      "Epoch 47/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0145 - acc: 0.9960 - val_loss: 2.9227 - val_acc: 0.7453\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00945\n",
      "Epoch 48/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0145 - acc: 0.9958 - val_loss: 2.9062 - val_acc: 0.7417\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00945\n",
      "Epoch 49/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0136 - acc: 0.9961 - val_loss: 2.7377 - val_acc: 0.7627\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00945\n",
      "Epoch 50/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0137 - acc: 0.9962 - val_loss: 2.9099 - val_acc: 0.7452\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.00945\n",
      "Epoch 51/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0135 - acc: 0.9962 - val_loss: 2.9476 - val_acc: 0.7431\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.00945\n",
      "Epoch 52/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0136 - acc: 0.9961 - val_loss: 2.7359 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.00945\n",
      "Epoch 53/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0133 - acc: 0.9963 - val_loss: 2.9639 - val_acc: 0.7515\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.00945\n",
      "Epoch 54/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0140 - acc: 0.9961 - val_loss: 2.9569 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.00945\n",
      "Epoch 55/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0137 - acc: 0.9961 - val_loss: 2.7982 - val_acc: 0.7570\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.00945\n",
      "Epoch 56/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0135 - acc: 0.9961 - val_loss: 2.8134 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.00945\n",
      "Epoch 57/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0128 - acc: 0.9964 - val_loss: 2.9199 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.00945\n",
      "Epoch 58/100\n",
      "125973/125973 [==============================] - 7s 59us/step - loss: 0.0130 - acc: 0.9963 - val_loss: 2.9828 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.00945\n",
      "Epoch 59/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0130 - acc: 0.9964 - val_loss: 2.9990 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.00945\n",
      "Epoch 60/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0129 - acc: 0.9963 - val_loss: 3.0378 - val_acc: 0.7401\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.00945\n",
      "Epoch 61/100\n",
      "125973/125973 [==============================] - 7s 53us/step - loss: 0.0133 - acc: 0.9964 - val_loss: 2.8712 - val_acc: 0.7632\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.00945\n",
      "Epoch 62/100\n",
      "125973/125973 [==============================] - 7s 53us/step - loss: 0.0130 - acc: 0.9962 - val_loss: 2.9783 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.00945\n",
      "Epoch 63/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0126 - acc: 0.9964 - val_loss: 3.1396 - val_acc: 0.7411\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.00945\n",
      "Epoch 64/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0123 - acc: 0.9964 - val_loss: 2.9969 - val_acc: 0.7350\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.00945\n",
      "Epoch 65/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0128 - acc: 0.9962 - val_loss: 3.0803 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.00945\n",
      "Epoch 66/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0124 - acc: 0.9966 - val_loss: 2.9752 - val_acc: 0.7508\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.00945\n",
      "Epoch 67/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0121 - acc: 0.9966 - val_loss: 3.0002 - val_acc: 0.7565\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.00945\n",
      "Epoch 68/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0122 - acc: 0.9965 - val_loss: 3.0510 - val_acc: 0.7538\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.00945\n",
      "Epoch 69/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0120 - acc: 0.9966 - val_loss: 3.3252 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.00945\n",
      "Epoch 70/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0123 - acc: 0.9965 - val_loss: 3.1556 - val_acc: 0.7528\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.00945\n",
      "Epoch 71/100\n",
      "125973/125973 [==============================] - 7s 52us/step - loss: 0.0123 - acc: 0.9966 - val_loss: 3.0018 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.00945\n",
      "Epoch 72/100\n",
      "125973/125973 [==============================] - 6s 50us/step - loss: 0.0121 - acc: 0.9966 - val_loss: 2.9914 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.00945\n",
      "Epoch 73/100\n",
      "125973/125973 [==============================] - 6s 48us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 3.0517 - val_acc: 0.7424\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.00945\n",
      "Epoch 74/100\n",
      "125973/125973 [==============================] - 6s 50us/step - loss: 0.0121 - acc: 0.9965 - val_loss: 2.9431 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.00945\n",
      "Epoch 75/100\n",
      "125973/125973 [==============================] - 6s 50us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 2.9031 - val_acc: 0.7642\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.00945\n",
      "Epoch 76/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 3.0692 - val_acc: 0.7468\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.00945\n",
      "Epoch 77/100\n",
      "125973/125973 [==============================] - 8s 60us/step - loss: 0.0121 - acc: 0.9967 - val_loss: 2.9851 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.00945\n",
      "Epoch 78/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0116 - acc: 0.9967 - val_loss: 3.1748 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.00945\n",
      "Epoch 79/100\n",
      "125973/125973 [==============================] - 8s 63us/step - loss: 0.0119 - acc: 0.9967 - val_loss: 2.9689 - val_acc: 0.7642\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.00945\n",
      "Epoch 80/100\n",
      "125973/125973 [==============================] - 8s 61us/step - loss: 0.0118 - acc: 0.9967 - val_loss: 2.8971 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.00945\n",
      "Epoch 81/100\n",
      "125973/125973 [==============================] - 8s 61us/step - loss: 0.0115 - acc: 0.9967 - val_loss: 3.0645 - val_acc: 0.7662\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.00945\n",
      "Epoch 82/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 3.0308 - val_acc: 0.7590\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.00945\n",
      "Epoch 83/100\n",
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0120 - acc: 0.9968 - val_loss: 3.0863 - val_acc: 0.7455\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.00945\n",
      "Epoch 84/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0120 - acc: 0.9968 - val_loss: 3.0125 - val_acc: 0.7419\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.00945\n",
      "Epoch 85/100\n",
      "125973/125973 [==============================] - 7s 55us/step - loss: 0.0116 - acc: 0.9968 - val_loss: 3.1068 - val_acc: 0.7530\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.00945\n",
      "Epoch 86/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0119 - acc: 0.9967 - val_loss: 2.9143 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.00945\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973/125973 [==============================] - 7s 54us/step - loss: 0.0109 - acc: 0.9968 - val_loss: 2.9877 - val_acc: 0.7529\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.00945\n",
      "Epoch 88/100\n",
      "125973/125973 [==============================] - 8s 61us/step - loss: 0.0115 - acc: 0.9968 - val_loss: 3.1293 - val_acc: 0.7565\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.00945\n",
      "Epoch 89/100\n",
      "125973/125973 [==============================] - 8s 60us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 3.0981 - val_acc: 0.7523\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.00945\n",
      "Epoch 90/100\n",
      "125973/125973 [==============================] - 7s 59us/step - loss: 0.0115 - acc: 0.9968 - val_loss: 3.0658 - val_acc: 0.7519\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.00945\n",
      "Epoch 91/100\n",
      "125973/125973 [==============================] - 7s 58us/step - loss: 0.0113 - acc: 0.9969 - val_loss: 3.1233 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.00945\n",
      "Epoch 92/100\n",
      "125973/125973 [==============================] - 7s 57us/step - loss: 0.0115 - acc: 0.9969 - val_loss: 3.1807 - val_acc: 0.7456\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.00945\n",
      "Epoch 93/100\n",
      "125973/125973 [==============================] - 7s 56us/step - loss: 0.0109 - acc: 0.9967 - val_loss: 3.1913 - val_acc: 0.7444\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.00945\n",
      "Epoch 94/100\n",
      "125973/125973 [==============================] - 7s 59us/step - loss: 0.0111 - acc: 0.9969 - val_loss: 2.9920 - val_acc: 0.7461\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.00945\n",
      "Epoch 95/100\n",
      "125973/125973 [==============================] - 8s 61us/step - loss: 0.0111 - acc: 0.9969 - val_loss: 3.0314 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.00945\n",
      "Epoch 96/100\n",
      "125973/125973 [==============================] - 7s 60us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 3.0700 - val_acc: 0.7447\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.00945\n",
      "Epoch 97/100\n",
      "125973/125973 [==============================] - 7s 59us/step - loss: 0.0118 - acc: 0.9969 - val_loss: 2.9533 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.00945\n",
      "Epoch 98/100\n",
      "125973/125973 [==============================] - 7s 59us/step - loss: 0.0109 - acc: 0.9969 - val_loss: 3.0904 - val_acc: 0.7568\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.00945\n",
      "Epoch 99/100\n",
      "125973/125973 [==============================] - 8s 60us/step - loss: 0.0115 - acc: 0.9968 - val_loss: 3.0874 - val_acc: 0.7571\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.00945\n",
      "Epoch 100/100\n",
      "125973/125973 [==============================] - 7s 59us/step - loss: 0.0107 - acc: 0.9968 - val_loss: 3.0265 - val_acc: 0.7571\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.00945\n"
     ]
    }
   ],
   "source": [
    "# definitions for kb_5 (30 features)\n",
    "model_kb5 = Sequential()\n",
    "model_kb5.add(Dense(1024, \n",
    "                    input_dim = X_train_kb5.shape[1], \n",
    "                    activation = 'relu'))  \n",
    "model_kb5.add(Dropout(0.1))\n",
    "model_kb5.add(Dense(128, \n",
    "                    activation = 'relu'))  \n",
    "model_kb5.add(Dropout(0.2))\n",
    "model_kb5.add(Dense(5))\n",
    "model_kb5.add(Activation('softmax'))\n",
    "\n",
    "# optimizers and configs\n",
    "model_kb5.compile(loss = 'categorical_crossentropy',\n",
    "                    optimizer = 'adam',\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "# checkpointer = callbacks.ModelCheckpoint(filepath = \"dnn/checkpoints/checkpoint_kb5-{epoch:02d}.hdf5\",\n",
    "#                                          verbose = 1, \n",
    "#                                          save_best_only = True, \n",
    "#                                          monitor = 'loss')\n",
    "\n",
    "csv_logger = CSVLogger('dnn/dnn_kb5_analysis.csv',\n",
    "                       separator = ',', \n",
    "                       append = False)\n",
    "\n",
    "model_kb5.fit(X_train_kb5, y_train5, \n",
    "              validation_data = (X_test_kb5, y_test5),\n",
    "              batch_size = batch_size, \n",
    "              epochs = epochs, \n",
    "              callbacks = [checkpointer, csv_logger])\n",
    "\n",
    "model_kb5.save('dnn/dnn_model_kb5.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various classifiers using ACCURACY score:\n",
    "\n",
    "|  Classifier       | Probe    | DOS      | UR2      | R2L      |Binary Class|MultiClass|\n",
    "|-------------      |-------   |------    |------    |------    |------------|----------| \n",
    "|**Decision Tree**  | 99.37%   |99.62%    |99.65%    |97.49%    |   98.01%   |  97.82%  |\n",
    "|Best Features      | 99.07%   |99.60%    |99.03%    |97.48%    |   97.91%   |  97.20%  |\n",
    "|                                                                                       |\n",
    "|**KNN**            | 98.90%   |99.11%    |99.11%    |97.27%    |   97.24%   |  96.73%  |\n",
    "|Best Features      | 98.54%   |99.02%    |98.63%    |97.15%    |   97.23%   |  96.32%  |\n",
    "|                                                                                       |\n",
    "|**SVM**            | 97.77%   |96.83%    |99.07%    |95.23%    |   92.81%   |  93.46%  |\n",
    "|Best Features      | 97.32%   |96.21%    |97.98%    |92.60%    |   91.63%   |  91.40%  |\n",
    "|                                                                                       |\n",
    "|**Nayve Bayes**    | 87.26%   |84.75%    |97.98%    |77.75%    |   82.41%   |  64.56%  |\n",
    "|Best Features      | 87.00%   |84.57%    |97.98%    |77.73%    |   82.34%   |  64.50%  |\n",
    "|                                                                                       |\n",
    "|**Adaboost**       | 99.69%   |**99.84%**|**99.79%**|**98.06%**|   98.54%   |  98.42%  |\n",
    "|Best Features      |**99.84%**|99.79%    |99.29%    |97.90%    |   98.35%   |  98.23%  |\n",
    "|                                                                                       |\n",
    "|**Random Forest**  | 99.53%   |99.81%    |**99.79%**|97.87%    |   98.54%   |  98.24%  |\n",
    "|Best Features      | 99.81%   |99.79%    |99.20%    |98.03%    |   98.27%   |  98.06%  |\n",
    "|                                                                                       |\n",
    "|**Deep Neural Net**|    -     |   -      |  -       |    -     | **99.77%** |**99.77%**|\n",
    "|Best Features      |    -     |   -      |  -       |    -     |   99.72%   |  99.70%  |\n",
    "|                                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best overall results were obtained by the **deep learning models**.\n",
    "<br>\n",
    "<br>The second best, were followed by **ensemble methods** (Adaboost first and Random Forest second).\n",
    "<br>For the classic classifiers, we had **Decision Tree** as the best approach.<br>\n",
    "\n",
    "For the trading between accuracy and processing time, due to dimensionality reduction, we can afford to lose around 0,06% of accuracy while handling with less complex dataset, with a very good gain of computation time.\n",
    "--- -- \n",
    "For the future works, we can consider using other techniques for feature selection, like 'feature ratio', 'random projection' and 'principal component analysis', all for dimensionality reduction. Another neural networks approaches can also be considered, like LSTM, GRU and RNN.\n",
    "--- --\n",
    "Despite the use of high amounts of time and data to train the model, on the real life problems, these models (mainly DNN), when loaded, can be applied with real time processing results. They would run with updates over the air, and the data collected among the many points would be used to improve accuracy on all devices. It can work together with a logs consolidator. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
